{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def print_result(predict_label, id2rel, start_idx=8001):\n",
    "    with open('predicted_result.txt', 'w', encoding='utf-8') as fw:\n",
    "        for i in range(0, predict_label.shape[0]):\n",
    "            fw.write('{}\\t{}\\n'.format(\n",
    "                start_idx+i, id2rel[int(predict_label[i])]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [],
   "source": [
    "def train(model, criterion, loader, config):\n",
    "    train_loader, dev_loader, _ = loader\n",
    "    optimizer = optim.Adam(model.parameters(), lr=config.lr,\n",
    "                           weight_decay=config.L2_decay)\n",
    "\n",
    "    print(model)\n",
    "    print('traning model parameters:')\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.requires_grad:\n",
    "            print('%s :  %s' % (name, str(param.data.shape)))\n",
    "    print('--------------------------------------')\n",
    "    print('start to train the model ...')\n",
    "\n",
    "    eval_tool = Eval(config)\n",
    "    max_f1 = -float('inf')\n",
    "    for epoch in range(1, config.epoch+1):\n",
    "        for step, (data, label) in enumerate(train_loader):\n",
    "            model.train()\n",
    "            data = data.to(config.device)\n",
    "            label = label.to(config.device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(data)\n",
    "            loss = criterion(logits, label)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        _, train_loss, _ = eval_tool.evaluate(model, criterion, train_loader)\n",
    "        f1, dev_loss, _ = eval_tool.evaluate(model, criterion, dev_loader)\n",
    "\n",
    "        print('[%03d] train_loss: %.3f | dev_loss: %.3f | micro f1 on dev: %.4f'\n",
    "              % (epoch, train_loss, dev_loss, f1), end=' ')\n",
    "        if f1 > max_f1:\n",
    "            max_f1 = f1\n",
    "            torch.save(model.state_dict(), os.path.join(\n",
    "                config.model_dir, 'model.pkl'))\n",
    "            print('>>> save models!')\n",
    "        else:\n",
    "            print()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [],
   "source": [
    "def test(model, criterion, loader, config):\n",
    "    print('--------------------------------------')\n",
    "    print('start test ...')\n",
    "\n",
    "    _, _, test_loader = loader\n",
    "    model.load_state_dict(torch.load(\n",
    "        os.path.join(config.model_dir, 'model.pkl')))\n",
    "    eval_tool = Eval(config)\n",
    "    f1, test_loss, predict_label = eval_tool.evaluate(\n",
    "        model, criterion, test_loader)\n",
    "    print('test_loss: %.3f | micro f1 on test:  %.4f' % (test_loss, f1))\n",
    "    return predict_label\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "outputs": [],
   "source": [
    "#train dataset simeval2010"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "some config:\n",
      "data_dir = ./data\n",
      "output_dir = ./output\n",
      "embedding_path = ./embedding/hlbl-embeddings-scaled.EMBEDDING_SIZE=50.txt\n",
      "word_dim = 50\n",
      "model_name = CNN\n",
      "mode = 1\n",
      "seed = 5782\n",
      "cuda = -1\n",
      "epoch = 20\n",
      "dropout = 0.5\n",
      "batch_size = 128\n",
      "lr = 0.001\n",
      "max_len = 100\n",
      "pos_dis = 50\n",
      "pos_dim = 5\n",
      "hidden_size = 100\n",
      "filter_num = 200\n",
      "window = 3\n",
      "L2_decay = 1e-05\n",
      "device = cpu\n",
      "model_dir = ./output/CNN\n",
      "--------------------------------------\n",
      "start to load data ...\n",
      "finish!\n",
      "--------------------------------------\n",
      "CNN(\n",
      "  (word_embedding): Embedding(246123, 50)\n",
      "  (pos1_embedding): Embedding(103, 5)\n",
      "  (pos2_embedding): Embedding(103, 5)\n",
      "  (conv): Conv2d(1, 200, kernel_size=(3, 60), stride=(1, 1), padding=(1, 0))\n",
      "  (maxpool): MaxPool2d(kernel_size=(100, 1), stride=(100, 1), padding=0, dilation=1, ceil_mode=False)\n",
      "  (tanh): Tanh()\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (linear): Linear(in_features=200, out_features=100, bias=True)\n",
      "  (dense): Linear(in_features=100, out_features=19, bias=True)\n",
      ")\n",
      "traning model parameters:\n",
      "word_embedding.weight :  torch.Size([246123, 50])\n",
      "pos1_embedding.weight :  torch.Size([103, 5])\n",
      "pos2_embedding.weight :  torch.Size([103, 5])\n",
      "conv.weight :  torch.Size([200, 1, 3, 60])\n",
      "conv.bias :  torch.Size([200])\n",
      "linear.weight :  torch.Size([100, 200])\n",
      "linear.bias :  torch.Size([100])\n",
      "dense.weight :  torch.Size([19, 100])\n",
      "dense.bias :  torch.Size([19])\n",
      "--------------------------------------\n",
      "start to train the model ...\n",
      "[001] train_loss: 1.057 | dev_loss: 1.059 | micro f1 on dev: 0.0000 >>> save models!\n",
      "[002] train_loss: 0.422 | dev_loss: 0.417 | micro f1 on dev: 0.0000 \n",
      "[003] train_loss: 0.395 | dev_loss: 0.385 | micro f1 on dev: 0.0000 \n",
      "[004] train_loss: 0.292 | dev_loss: 0.279 | micro f1 on dev: 0.0000 \n",
      "[005] train_loss: 0.291 | dev_loss: 0.277 | micro f1 on dev: 0.0000 \n",
      "[006] train_loss: 0.272 | dev_loss: 0.262 | micro f1 on dev: 0.0000 \n",
      "[007] train_loss: 0.271 | dev_loss: 0.265 | micro f1 on dev: 0.0000 \n",
      "[008] train_loss: 0.258 | dev_loss: 0.255 | micro f1 on dev: 0.0000 \n",
      "[009] train_loss: 0.250 | dev_loss: 0.250 | micro f1 on dev: 0.0000 \n",
      "[010] train_loss: 0.241 | dev_loss: 0.244 | micro f1 on dev: 0.0000 \n",
      "[011] train_loss: 0.229 | dev_loss: 0.236 | micro f1 on dev: 0.0000 \n",
      "[012] train_loss: 0.209 | dev_loss: 0.223 | micro f1 on dev: 0.0000 \n",
      "[013] train_loss: 0.185 | dev_loss: 0.206 | micro f1 on dev: 0.0000 \n",
      "[014] train_loss: 0.157 | dev_loss: 0.186 | micro f1 on dev: 0.0465 >>> save models!\n",
      "[015] train_loss: 0.127 | dev_loss: 0.165 | micro f1 on dev: 0.3137 >>> save models!\n",
      "[016] train_loss: 0.103 | dev_loss: 0.151 | micro f1 on dev: 0.4848 >>> save models!\n",
      "[017] train_loss: 0.084 | dev_loss: 0.145 | micro f1 on dev: 0.4776 \n",
      "[018] train_loss: 0.069 | dev_loss: 0.145 | micro f1 on dev: 0.5278 >>> save models!\n",
      "[019] train_loss: 0.059 | dev_loss: 0.147 | micro f1 on dev: 0.5195 \n",
      "[020] train_loss: 0.047 | dev_loss: 0.151 | micro f1 on dev: 0.5000 \n",
      "--------------------------------------\n",
      "start test ...\n",
      "test_loss: 0.145 | micro f1 on test:  0.5278\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from config import Config\n",
    "from utils import WordEmbeddingLoader, RelationLoader, SemEvalDataLoader\n",
    "from model import CNN\n",
    "from evaluate import Eval\n",
    "config = Config()\n",
    "print('--------------------------------------')\n",
    "print('some config:')\n",
    "config.print_config()\n",
    "\n",
    "print('--------------------------------------')\n",
    "print('start to load data ...')\n",
    "word2id, word_vec = WordEmbeddingLoader(config).load_embedding()\n",
    "rel2id, id2rel, class_num = RelationLoader(config).get_relation()\n",
    "loader = SemEvalDataLoader(rel2id, word2id, config)\n",
    "\n",
    "train_loader, dev_loader = None, None\n",
    "if config.mode == 1:  # train mode\n",
    "    train_loader = loader.get_train()\n",
    "    dev_loader = loader.get_dev()\n",
    "test_loader = loader.get_test()\n",
    "loader = [train_loader, dev_loader, test_loader]\n",
    "print('finish!')\n",
    "\n",
    "print('--------------------------------------')\n",
    "model = CNN(word_vec=word_vec, class_num=class_num, config=config)\n",
    "model = model.to(config.device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "if config.mode == 1:  # train mode\n",
    "    train(model, criterion, loader, config)\n",
    "predict_label = test(model, criterion, loader, config)\n",
    "print_result(predict_label, id2rel)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}