{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# CNN Approach\n",
    "Based on [Zeng et al.](https://aclanthology.org/C14-1220.pdf) and implementation from [here](https://github.com/onehaitao/CNN-relation-extraction).\n",
    "\n",
    "The code is written for the evaluation of SemEval2010. Change the loader to get the results for SimEval2007."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Training loop"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "def train(model, criterion, loader, config):\n",
    "    train_loader, dev_loader, _ = loader\n",
    "    optimizer = optim.Adam(model.parameters(), lr=config.lr,\n",
    "                           weight_decay=config.L2_decay)\n",
    "\n",
    "    print(model)\n",
    "    print('traning model parameters:')\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.requires_grad:\n",
    "            print('%s :  %s' % (name, str(param.data.shape)))\n",
    "    print('--------------------------------------')\n",
    "    print('start to train the model ...')\n",
    "\n",
    "    eval_tool = Eval(config)\n",
    "    max_f1 = -float('inf')\n",
    "    for epoch in range(1, config.epoch+1):\n",
    "        for step, (data, label) in enumerate(train_loader):\n",
    "            model.train()\n",
    "            data = data.to(config.device)\n",
    "            label = label.to(config.device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(data)\n",
    "            loss = criterion(logits, label)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        _, train_loss, _ = eval_tool.evaluate(model, criterion, train_loader)\n",
    "        f1, dev_loss, _ = eval_tool.evaluate(model, criterion, dev_loader)\n",
    "\n",
    "        print('[%03d] train_loss: %.3f | dev_loss: %.3f | micro f1 on dev: %.4f'\n",
    "              % (epoch, train_loss, dev_loss, f1), end=' ')\n",
    "        if f1 > max_f1:\n",
    "            max_f1 = f1\n",
    "            torch.save(model.state_dict(), os.path.join(\n",
    "                config.model_dir, 'model.pkl'))\n",
    "            print('>>> save models!')\n",
    "        else:\n",
    "            print()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Test evaluation during training"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "def test(model, criterion, loader, config):\n",
    "    print('--------------------------------------')\n",
    "    print('start test ...')\n",
    "\n",
    "    _, _, test_loader = loader\n",
    "    model.load_state_dict(torch.load(\n",
    "        os.path.join(config.model_dir, 'model.pkl')))\n",
    "    eval_tool = Eval(config)\n",
    "    f1, test_loss, predict_label = eval_tool.evaluate(\n",
    "        model, criterion, test_loader)\n",
    "    print('test_loss: %.3f | micro f1 on test:  %.4f' % (test_loss, f1))\n",
    "    return predict_label\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Prepare custom statistics with metrics from the [survey](https://link.springer.com/content/pdf/10.1007/s10115-022-01665-w.pdf).\n",
    "They are also used for the baseline"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dmitrii/anaconda3/envs/psdaub4/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from CNN.config import Config\n",
    "from CNN.utils import WordEmbeddingLoader, RelationLoader, SemEvalDataLoader\n",
    "from CNN.model import CNN\n",
    "from CNN.evaluate import Eval\n",
    "config_test = Config()\n",
    "config_test.batch_size = 1\n",
    "config_test.embedding_path = \"./CNN/embedding/hlbl-embeddings-scaled.EMBEDDING_SIZE=50.txt\"\n",
    "config_test.data_dir = \"./CNN/data/simeval2010/\"  # change to simeval2007 to get another loader\n",
    "word2id, word_vec = WordEmbeddingLoader(config_test).load_embedding()\n",
    "rel2id, id2rel, class_num = RelationLoader(config_test).get_relation()\n",
    "loader = SemEvalDataLoader(rel2id, word2id, config_test)\n",
    "def get_x_y_from_loader(loader):\n",
    "    # upload train and test from dataloader\n",
    "    X = []\n",
    "    y = []\n",
    "    for step, (data, label) in enumerate(loader):\n",
    "        x = data.detach().numpy().flatten()\n",
    "        x.astype(int)\n",
    "        X.append(x)\n",
    "        y.append(label.detach().numpy()[0])\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    return X, y\n",
    "X_test, y_test = get_x_y_from_loader(loader.get_test())\n",
    "from custom_statistics import Statistics\n",
    "stats = Statistics()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Train CNN model with different states (the same as for the baseline)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "some config:\n",
      "data_dir = ./CNN/data/simeval2010/\n",
      "output_dir = ./output\n",
      "embedding_path = ./CNN/embedding/hlbl-embeddings-scaled.EMBEDDING_SIZE=50.txt\n",
      "word_dim = 50\n",
      "model_name = CNN\n",
      "mode = 1\n",
      "seed = 5782\n",
      "cuda = -1\n",
      "epoch = 20\n",
      "dropout = 0.5\n",
      "batch_size = 128\n",
      "lr = 0.001\n",
      "max_len = 100\n",
      "pos_dis = 50\n",
      "pos_dim = 5\n",
      "hidden_size = 100\n",
      "filter_num = 200\n",
      "window = 3\n",
      "L2_decay = 1e-05\n",
      "device = cpu\n",
      "model_dir = ./output/CNN\n",
      "--------------------------------------\n",
      "start to load data ...\n",
      "finish!\n",
      "--------------------------------------\n",
      "RANDOM_SEED ->  0\n",
      "CNN(\n",
      "  (word_embedding): Embedding(246123, 50)\n",
      "  (pos1_embedding): Embedding(103, 5)\n",
      "  (pos2_embedding): Embedding(103, 5)\n",
      "  (conv): Conv2d(1, 200, kernel_size=(3, 60), stride=(1, 1), padding=(1, 0))\n",
      "  (maxpool): MaxPool2d(kernel_size=(100, 1), stride=(100, 1), padding=0, dilation=1, ceil_mode=False)\n",
      "  (tanh): Tanh()\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (linear): Linear(in_features=200, out_features=100, bias=True)\n",
      "  (dense): Linear(in_features=100, out_features=19, bias=True)\n",
      ")\n",
      "traning model parameters:\n",
      "word_embedding.weight :  torch.Size([246123, 50])\n",
      "pos1_embedding.weight :  torch.Size([103, 5])\n",
      "pos2_embedding.weight :  torch.Size([103, 5])\n",
      "conv.weight :  torch.Size([200, 1, 3, 60])\n",
      "conv.bias :  torch.Size([200])\n",
      "linear.weight :  torch.Size([100, 200])\n",
      "linear.bias :  torch.Size([100])\n",
      "dense.weight :  torch.Size([19, 100])\n",
      "dense.bias :  torch.Size([19])\n",
      "--------------------------------------\n",
      "start to train the model ...\n",
      "[001] train_loss: 0.427 | dev_loss: 0.422 | micro f1 on dev: 0.0000 >>> save models!\n",
      "[002] train_loss: 0.246 | dev_loss: 0.230 | micro f1 on dev: 0.4843 >>> save models!\n",
      "[003] train_loss: 0.145 | dev_loss: 0.148 | micro f1 on dev: 0.8210 >>> save models!\n",
      "[004] train_loss: 0.101 | dev_loss: 0.126 | micro f1 on dev: 0.8677 >>> save models!\n",
      "[005] train_loss: 0.070 | dev_loss: 0.109 | micro f1 on dev: 0.8654 \n",
      "[006] train_loss: 0.048 | dev_loss: 0.107 | micro f1 on dev: 0.8720 >>> save models!\n",
      "[007] train_loss: 0.034 | dev_loss: 0.110 | micro f1 on dev: 0.8792 >>> save models!\n",
      "[008] train_loss: 0.025 | dev_loss: 0.110 | micro f1 on dev: 0.8780 \n",
      "[009] train_loss: 0.015 | dev_loss: 0.111 | micro f1 on dev: 0.8805 >>> save models!\n",
      "[010] train_loss: 0.012 | dev_loss: 0.123 | micro f1 on dev: 0.8740 \n",
      "[011] train_loss: 0.007 | dev_loss: 0.120 | micro f1 on dev: 0.8844 >>> save models!\n",
      "[012] train_loss: 0.005 | dev_loss: 0.124 | micro f1 on dev: 0.8771 \n",
      "[013] train_loss: 0.003 | dev_loss: 0.125 | micro f1 on dev: 0.8794 \n",
      "[014] train_loss: 0.002 | dev_loss: 0.129 | micro f1 on dev: 0.8785 \n",
      "[015] train_loss: 0.002 | dev_loss: 0.138 | micro f1 on dev: 0.8735 \n",
      "[016] train_loss: 0.002 | dev_loss: 0.136 | micro f1 on dev: 0.8758 \n",
      "[017] train_loss: 0.001 | dev_loss: 0.139 | micro f1 on dev: 0.8704 \n",
      "[018] train_loss: 0.001 | dev_loss: 0.151 | micro f1 on dev: 0.8726 \n",
      "[019] train_loss: 0.001 | dev_loss: 0.158 | micro f1 on dev: 0.8597 \n",
      "[020] train_loss: 0.001 | dev_loss: 0.155 | micro f1 on dev: 0.8681 \n",
      "--------------------------------------\n",
      "start test ...\n",
      "test_loss: 0.120 | micro f1 on test:  0.8844\n",
      "RANDOM_SEED ->  1\n",
      "CNN(\n",
      "  (word_embedding): Embedding(246123, 50)\n",
      "  (pos1_embedding): Embedding(103, 5)\n",
      "  (pos2_embedding): Embedding(103, 5)\n",
      "  (conv): Conv2d(1, 200, kernel_size=(3, 60), stride=(1, 1), padding=(1, 0))\n",
      "  (maxpool): MaxPool2d(kernel_size=(100, 1), stride=(100, 1), padding=0, dilation=1, ceil_mode=False)\n",
      "  (tanh): Tanh()\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (linear): Linear(in_features=200, out_features=100, bias=True)\n",
      "  (dense): Linear(in_features=100, out_features=19, bias=True)\n",
      ")\n",
      "traning model parameters:\n",
      "word_embedding.weight :  torch.Size([246123, 50])\n",
      "pos1_embedding.weight :  torch.Size([103, 5])\n",
      "pos2_embedding.weight :  torch.Size([103, 5])\n",
      "conv.weight :  torch.Size([200, 1, 3, 60])\n",
      "conv.bias :  torch.Size([200])\n",
      "linear.weight :  torch.Size([100, 200])\n",
      "linear.bias :  torch.Size([100])\n",
      "dense.weight :  torch.Size([19, 100])\n",
      "dense.bias :  torch.Size([19])\n",
      "--------------------------------------\n",
      "start to train the model ...\n",
      "[001] train_loss: 0.170 | dev_loss: 0.197 | micro f1 on dev: 0.5213 >>> save models!\n",
      "[002] train_loss: 0.066 | dev_loss: 0.123 | micro f1 on dev: 0.8598 >>> save models!\n",
      "[003] train_loss: 0.039 | dev_loss: 0.122 | micro f1 on dev: 0.8634 >>> save models!\n",
      "[004] train_loss: 0.024 | dev_loss: 0.126 | micro f1 on dev: 0.8644 >>> save models!\n",
      "[005] train_loss: 0.016 | dev_loss: 0.134 | micro f1 on dev: 0.8653 >>> save models!\n",
      "[006] train_loss: 0.010 | dev_loss: 0.140 | micro f1 on dev: 0.8594 \n",
      "[007] train_loss: 0.008 | dev_loss: 0.144 | micro f1 on dev: 0.8646 \n",
      "[008] train_loss: 0.004 | dev_loss: 0.148 | micro f1 on dev: 0.8651 \n",
      "[009] train_loss: 0.003 | dev_loss: 0.154 | micro f1 on dev: 0.8626 \n",
      "[010] train_loss: 0.002 | dev_loss: 0.154 | micro f1 on dev: 0.8629 \n",
      "[011] train_loss: 0.001 | dev_loss: 0.158 | micro f1 on dev: 0.8571 \n",
      "[012] train_loss: 0.001 | dev_loss: 0.162 | micro f1 on dev: 0.8606 \n",
      "[013] train_loss: 0.001 | dev_loss: 0.165 | micro f1 on dev: 0.8661 >>> save models!\n",
      "[014] train_loss: 0.001 | dev_loss: 0.170 | micro f1 on dev: 0.8584 \n",
      "[015] train_loss: 0.000 | dev_loss: 0.170 | micro f1 on dev: 0.8669 >>> save models!\n",
      "[016] train_loss: 0.000 | dev_loss: 0.172 | micro f1 on dev: 0.8649 \n",
      "[017] train_loss: 0.000 | dev_loss: 0.174 | micro f1 on dev: 0.8664 \n",
      "[018] train_loss: 0.000 | dev_loss: 0.175 | micro f1 on dev: 0.8729 >>> save models!\n",
      "[019] train_loss: 0.000 | dev_loss: 0.179 | micro f1 on dev: 0.8677 \n",
      "[020] train_loss: 0.000 | dev_loss: 0.184 | micro f1 on dev: 0.8762 >>> save models!\n",
      "--------------------------------------\n",
      "start test ...\n",
      "test_loss: 0.184 | micro f1 on test:  0.8762\n",
      "RANDOM_SEED ->  42\n",
      "CNN(\n",
      "  (word_embedding): Embedding(246123, 50)\n",
      "  (pos1_embedding): Embedding(103, 5)\n",
      "  (pos2_embedding): Embedding(103, 5)\n",
      "  (conv): Conv2d(1, 200, kernel_size=(3, 60), stride=(1, 1), padding=(1, 0))\n",
      "  (maxpool): MaxPool2d(kernel_size=(100, 1), stride=(100, 1), padding=0, dilation=1, ceil_mode=False)\n",
      "  (tanh): Tanh()\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (linear): Linear(in_features=200, out_features=100, bias=True)\n",
      "  (dense): Linear(in_features=100, out_features=19, bias=True)\n",
      ")\n",
      "traning model parameters:\n",
      "word_embedding.weight :  torch.Size([246123, 50])\n",
      "pos1_embedding.weight :  torch.Size([103, 5])\n",
      "pos2_embedding.weight :  torch.Size([103, 5])\n",
      "conv.weight :  torch.Size([200, 1, 3, 60])\n",
      "conv.bias :  torch.Size([200])\n",
      "linear.weight :  torch.Size([100, 200])\n",
      "linear.bias :  torch.Size([100])\n",
      "dense.weight :  torch.Size([19, 100])\n",
      "dense.bias :  torch.Size([19])\n",
      "--------------------------------------\n",
      "start to train the model ...\n",
      "[001] train_loss: 0.131 | dev_loss: 0.175 | micro f1 on dev: 0.5581 >>> save models!\n",
      "[002] train_loss: 0.046 | dev_loss: 0.127 | micro f1 on dev: 0.8460 >>> save models!\n",
      "[003] train_loss: 0.023 | dev_loss: 0.132 | micro f1 on dev: 0.8571 >>> save models!\n",
      "[004] train_loss: 0.012 | dev_loss: 0.136 | micro f1 on dev: 0.8533 \n",
      "[005] train_loss: 0.006 | dev_loss: 0.143 | micro f1 on dev: 0.8535 \n",
      "[006] train_loss: 0.004 | dev_loss: 0.150 | micro f1 on dev: 0.8631 >>> save models!\n",
      "[007] train_loss: 0.002 | dev_loss: 0.154 | micro f1 on dev: 0.8469 \n",
      "[008] train_loss: 0.001 | dev_loss: 0.164 | micro f1 on dev: 0.8532 \n",
      "[009] train_loss: 0.001 | dev_loss: 0.171 | micro f1 on dev: 0.8483 \n",
      "[010] train_loss: 0.001 | dev_loss: 0.173 | micro f1 on dev: 0.8554 \n",
      "[011] train_loss: 0.001 | dev_loss: 0.176 | micro f1 on dev: 0.8482 \n",
      "[012] train_loss: 0.001 | dev_loss: 0.180 | micro f1 on dev: 0.8585 \n",
      "[013] train_loss: 0.001 | dev_loss: 0.184 | micro f1 on dev: 0.8576 \n",
      "[014] train_loss: 0.000 | dev_loss: 0.177 | micro f1 on dev: 0.8567 \n",
      "[015] train_loss: 0.000 | dev_loss: 0.182 | micro f1 on dev: 0.8636 >>> save models!\n",
      "[016] train_loss: 0.000 | dev_loss: 0.184 | micro f1 on dev: 0.8669 >>> save models!\n",
      "[017] train_loss: 0.000 | dev_loss: 0.191 | micro f1 on dev: 0.8545 \n",
      "[018] train_loss: 0.000 | dev_loss: 0.194 | micro f1 on dev: 0.8576 \n",
      "[019] train_loss: 0.000 | dev_loss: 0.202 | micro f1 on dev: 0.8558 \n",
      "[020] train_loss: 0.000 | dev_loss: 0.204 | micro f1 on dev: 0.8558 \n",
      "--------------------------------------\n",
      "start test ...\n",
      "test_loss: 0.184 | micro f1 on test:  0.8669\n",
      "RANDOM_SEED ->  100\n",
      "CNN(\n",
      "  (word_embedding): Embedding(246123, 50)\n",
      "  (pos1_embedding): Embedding(103, 5)\n",
      "  (pos2_embedding): Embedding(103, 5)\n",
      "  (conv): Conv2d(1, 200, kernel_size=(3, 60), stride=(1, 1), padding=(1, 0))\n",
      "  (maxpool): MaxPool2d(kernel_size=(100, 1), stride=(100, 1), padding=0, dilation=1, ceil_mode=False)\n",
      "  (tanh): Tanh()\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (linear): Linear(in_features=200, out_features=100, bias=True)\n",
      "  (dense): Linear(in_features=100, out_features=19, bias=True)\n",
      ")\n",
      "traning model parameters:\n",
      "word_embedding.weight :  torch.Size([246123, 50])\n",
      "pos1_embedding.weight :  torch.Size([103, 5])\n",
      "pos2_embedding.weight :  torch.Size([103, 5])\n",
      "conv.weight :  torch.Size([200, 1, 3, 60])\n",
      "conv.bias :  torch.Size([200])\n",
      "linear.weight :  torch.Size([100, 200])\n",
      "linear.bias :  torch.Size([100])\n",
      "dense.weight :  torch.Size([19, 100])\n",
      "dense.bias :  torch.Size([19])\n",
      "--------------------------------------\n",
      "start to train the model ...\n",
      "[001] train_loss: 0.131 | dev_loss: 0.171 | micro f1 on dev: 0.6997 >>> save models!\n",
      "[002] train_loss: 0.045 | dev_loss: 0.128 | micro f1 on dev: 0.8495 >>> save models!\n",
      "[003] train_loss: 0.019 | dev_loss: 0.135 | micro f1 on dev: 0.8607 >>> save models!\n",
      "[004] train_loss: 0.008 | dev_loss: 0.144 | micro f1 on dev: 0.8603 \n",
      "[005] train_loss: 0.004 | dev_loss: 0.154 | micro f1 on dev: 0.8519 \n",
      "[006] train_loss: 0.002 | dev_loss: 0.160 | micro f1 on dev: 0.8589 \n",
      "[007] train_loss: 0.001 | dev_loss: 0.169 | micro f1 on dev: 0.8505 \n",
      "[008] train_loss: 0.001 | dev_loss: 0.175 | micro f1 on dev: 0.8466 \n",
      "[009] train_loss: 0.001 | dev_loss: 0.181 | micro f1 on dev: 0.8407 \n",
      "[010] train_loss: 0.000 | dev_loss: 0.193 | micro f1 on dev: 0.8495 \n",
      "[011] train_loss: 0.001 | dev_loss: 0.191 | micro f1 on dev: 0.8598 \n",
      "[012] train_loss: 0.000 | dev_loss: 0.200 | micro f1 on dev: 0.8509 \n",
      "[013] train_loss: 0.000 | dev_loss: 0.200 | micro f1 on dev: 0.8571 \n",
      "[014] train_loss: 0.000 | dev_loss: 0.206 | micro f1 on dev: 0.8585 \n",
      "[015] train_loss: 0.000 | dev_loss: 0.204 | micro f1 on dev: 0.8558 \n",
      "[016] train_loss: 0.000 | dev_loss: 0.201 | micro f1 on dev: 0.8558 \n",
      "[017] train_loss: 0.000 | dev_loss: 0.205 | micro f1 on dev: 0.8531 \n",
      "[018] train_loss: 0.000 | dev_loss: 0.210 | micro f1 on dev: 0.8528 \n",
      "[019] train_loss: 0.000 | dev_loss: 0.220 | micro f1 on dev: 0.8511 \n",
      "[020] train_loss: 0.000 | dev_loss: 0.218 | micro f1 on dev: 0.8554 \n",
      "--------------------------------------\n",
      "start test ...\n",
      "test_loss: 0.135 | micro f1 on test:  0.8607\n",
      "RANDOM_SEED ->  5782\n",
      "CNN(\n",
      "  (word_embedding): Embedding(246123, 50)\n",
      "  (pos1_embedding): Embedding(103, 5)\n",
      "  (pos2_embedding): Embedding(103, 5)\n",
      "  (conv): Conv2d(1, 200, kernel_size=(3, 60), stride=(1, 1), padding=(1, 0))\n",
      "  (maxpool): MaxPool2d(kernel_size=(100, 1), stride=(100, 1), padding=0, dilation=1, ceil_mode=False)\n",
      "  (tanh): Tanh()\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (linear): Linear(in_features=200, out_features=100, bias=True)\n",
      "  (dense): Linear(in_features=100, out_features=19, bias=True)\n",
      ")\n",
      "traning model parameters:\n",
      "word_embedding.weight :  torch.Size([246123, 50])\n",
      "pos1_embedding.weight :  torch.Size([103, 5])\n",
      "pos2_embedding.weight :  torch.Size([103, 5])\n",
      "conv.weight :  torch.Size([200, 1, 3, 60])\n",
      "conv.bias :  torch.Size([200])\n",
      "linear.weight :  torch.Size([100, 200])\n",
      "linear.bias :  torch.Size([100])\n",
      "dense.weight :  torch.Size([19, 100])\n",
      "dense.bias :  torch.Size([19])\n",
      "--------------------------------------\n",
      "start to train the model ...\n",
      "[001] train_loss: 0.130 | dev_loss: 0.175 | micro f1 on dev: 0.6645 >>> save models!\n",
      "[002] train_loss: 0.055 | dev_loss: 0.133 | micro f1 on dev: 0.8369 >>> save models!\n",
      "[003] train_loss: 0.030 | dev_loss: 0.128 | micro f1 on dev: 0.8553 >>> save models!\n",
      "[004] train_loss: 0.015 | dev_loss: 0.139 | micro f1 on dev: 0.8558 >>> save models!\n",
      "[005] train_loss: 0.009 | dev_loss: 0.149 | micro f1 on dev: 0.8500 \n",
      "[006] train_loss: 0.005 | dev_loss: 0.160 | micro f1 on dev: 0.8463 \n",
      "[007] train_loss: 0.003 | dev_loss: 0.169 | micro f1 on dev: 0.8585 >>> save models!\n",
      "[008] train_loss: 0.001 | dev_loss: 0.176 | micro f1 on dev: 0.8463 \n",
      "[009] train_loss: 0.001 | dev_loss: 0.181 | micro f1 on dev: 0.8527 \n",
      "[010] train_loss: 0.001 | dev_loss: 0.196 | micro f1 on dev: 0.8398 \n",
      "[011] train_loss: 0.001 | dev_loss: 0.196 | micro f1 on dev: 0.8463 \n",
      "[012] train_loss: 0.000 | dev_loss: 0.201 | micro f1 on dev: 0.8459 \n",
      "[013] train_loss: 0.001 | dev_loss: 0.221 | micro f1 on dev: 0.8306 \n",
      "[014] train_loss: 0.000 | dev_loss: 0.208 | micro f1 on dev: 0.8537 \n",
      "[015] train_loss: 0.000 | dev_loss: 0.211 | micro f1 on dev: 0.8404 \n",
      "[016] train_loss: 0.000 | dev_loss: 0.208 | micro f1 on dev: 0.8515 \n",
      "[017] train_loss: 0.000 | dev_loss: 0.214 | micro f1 on dev: 0.8432 \n",
      "[018] train_loss: 0.000 | dev_loss: 0.219 | micro f1 on dev: 0.8450 \n",
      "[019] train_loss: 0.000 | dev_loss: 0.225 | micro f1 on dev: 0.8326 \n",
      "[020] train_loss: 0.000 | dev_loss: 0.229 | micro f1 on dev: 0.8333 \n",
      "--------------------------------------\n",
      "start test ...\n",
      "test_loss: 0.169 | micro f1 on test:  0.8585\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "config = Config()\n",
    "config.data_dir = \"./CNN/data/simeval2010/\"  # change to simeval2007 to get another loader\n",
    "config.embedding_path = \"./CNN/embedding/hlbl-embeddings-scaled.EMBEDDING_SIZE=50.txt\"\n",
    "print('--------------------------------------')\n",
    "print('some config:')\n",
    "config.print_config()\n",
    "\n",
    "print('--------------------------------------')\n",
    "print('start to load data ...')\n",
    "word2id, word_vec = WordEmbeddingLoader(config).load_embedding()\n",
    "rel2id, id2rel, class_num = RelationLoader(config).get_relation()\n",
    "loader = SemEvalDataLoader(rel2id, word2id, config)\n",
    "\n",
    "train_loader, dev_loader = None, None\n",
    "if config.mode == 1:  # train mode\n",
    "    train_loader = loader.get_train()\n",
    "    dev_loader = loader.get_dev()\n",
    "test_loader = loader.get_test()\n",
    "loader = [train_loader, dev_loader, test_loader]\n",
    "print('finish!')\n",
    "\n",
    "print('--------------------------------------')\n",
    "random_states = [0, 1, 42, 100, 5782]\n",
    "for random_state in random_states:\n",
    "    print(\"RANDOM_SEED -> \", random_state)\n",
    "    config.set_seed(random_state)\n",
    "    model = CNN(word_vec=word_vec, class_num=class_num, config=config)\n",
    "    model = model.to(config.device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    if config.mode == 1:  # train mode\n",
    "        train(model, criterion, loader, config)\n",
    "    predict_label = test(model, criterion, loader, config)\n",
    "    stats.add(predict_label, y_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score ->  (0.9704821494295178, 0.00216370091375768)\n",
      "Precision Score ->  (0.9700893024042265, 0.002196022384812887)\n",
      "Recall Score ->  (0.9704821494295178, 0.00216370091375768)\n",
      "F1 Score ->  (0.9702090923752783, 0.0021913229745233838)\n",
      "Matthews Correlation Coefficient ->  (0.8631673351739145, 0.010103334814723704)\n",
      "G Mean Score ->  (0.8936479809254095, 0.008554961392298987)\n"
     ]
    }
   ],
   "source": [
    "stats.show('weighted')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}