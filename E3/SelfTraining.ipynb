{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Self-training as pretraining\n",
    "The transformation are used as pretext tasks on unlabeled data to learn features.\n",
    "The learnt weights are used for initialization of weights.\n",
    "To see different transformation, look into Transformations notebook.\n",
    "Two types of self-training are tested:\n",
    "1. binary classification, whether the transformation of time series was applied\n",
    "2. multi-task classification, which transformation was applied"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Binary classification\n",
    "Prepare datasets"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/dmitrii/anaconda3/envs/work/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "from cnn1d_modules import Model, Dataset\n",
    "from transformation import identity, flip, permute, time_warp\n",
    "from statistics import Statistics"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PATH:  /home/dmitrii/GitHub/SDAPraktikum/E3/PAMAP2_Dataset/pamap2.h5\n",
      "x_train shape =  (128247, 52)\n",
      "y_train shape = (128247,)\n",
      "x_test shape = (22659, 52)\n",
      "y_test shape = (22659,)\n",
      "x_train shape(downsampled) =  (42749, 52)\n",
      "y_train shape(downsampled) = (42749,)\n",
      "x_test shape(downsampled) = (7553, 52)\n",
      "y_test shape(downsampled) = (7553,)\n",
      "[ 1  2  3  4  5  6  7 11 12 13 14 18]\n",
      "[ 1  2  3  4  5  6  7 11 12 13 14 18]\n",
      "segmenting signal...\n",
      "signal segmented.\n",
      "train_x shape = (3561, 25, 52)\n",
      "train_y shape = (3561,)\n",
      "test_x shape = (628, 25, 52)\n",
      "test_y shape = (628,)\n",
      "PATH:  /home/dmitrii/GitHub/SDAPraktikum/E3/OpportunityUCIDataset/opportunity.h5\n",
      "x_train shape =  (700165, 77)\n",
      "y_train shape = (700165,)\n",
      "x_test shape = (120516, 77)\n",
      "y_test shape = (120516,)\n",
      "[ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18]\n",
      "[ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18]\n",
      "segmenting signal...\n",
      "signal segmented.\n",
      "train_x shape = (63650, 23, 77)\n",
      "train_y shape = (63650,)\n",
      "test_x shape = (10955, 23, 77)\n",
      "test_y shape = (10955,)\n"
     ]
    },
    {
     "data": {
      "text/plain": "(array([[[-9.830e-01, -1.990e-01,  1.190e-01, ...,  2.000e-02,\n           4.200e-02,  1.750e-01],\n         [-9.860e-01, -2.200e-01,  1.140e-01, ...,  1.700e-02,\n           3.100e-02,  1.750e-01],\n         [-9.870e-01, -2.270e-01,  1.120e-01, ..., -2.700e-02,\n           1.500e-02,  1.750e-01],\n         ...,\n         [-9.720e-01, -1.420e-01,  6.900e-02, ..., -4.500e-02,\n           5.800e-02,  1.750e-01],\n         [-9.680e-01, -1.420e-01,  6.400e-02, ..., -1.600e-02,\n           4.400e-02,  1.750e-01],\n         [-9.650e-01, -1.450e-01,  8.700e-02, ...,  8.700e-02,\n           2.400e-02,  1.750e-01]],\n \n        [[-9.990e-01, -1.420e-01,  1.190e-01, ..., -4.000e-03,\n           9.000e-03,  1.750e-01],\n         [-9.710e-01, -1.370e-01,  9.000e-02, ..., -1.300e-02,\n           2.000e-02,  1.750e-01],\n         [-9.680e-01, -1.530e-01,  6.900e-02, ..., -7.000e-03,\n           1.500e-02,  1.750e-01],\n         ...,\n         [-9.640e-01, -1.050e-01,  1.700e-02, ..., -5.400e-02,\n          -3.459e+00,  1.760e-01],\n         [-9.770e-01, -1.250e-01,  3.000e-03, ...,  9.790e-01,\n          -5.223e+00,  1.740e-01],\n         [-9.880e-01, -1.520e-01,  4.700e-02, ..., -1.306e+00,\n          -2.782e+00,  1.720e-01]],\n \n        [[-9.650e-01, -1.450e-01,  8.700e-02, ...,  8.700e-02,\n           2.400e-02,  1.750e-01],\n         [-9.700e-01, -1.390e-01,  9.000e-02, ...,  3.800e-02,\n           4.200e-02,  1.750e-01],\n         [-9.870e-01, -1.300e-01,  8.000e-02, ...,  2.360e-01,\n           6.200e-02,  1.760e-01],\n         ...,\n         [-9.540e-01, -2.360e-01,  1.170e-01, ..., -5.100e-02,\n          -3.000e-03,  1.700e-01],\n         [-9.400e-01, -2.430e-01,  9.600e-02, ..., -1.400e-02,\n          -2.700e-02,  1.700e-01],\n         [-9.290e-01, -2.580e-01,  8.200e-02, ...,  1.300e-02,\n          -2.600e-02,  1.700e-01]],\n \n        ...,\n \n        [[-9.840e-01, -1.240e-01,  7.800e-02, ..., -1.900e-02,\n          -2.000e-02, -7.700e-02],\n         [-9.790e-01, -1.340e-01,  7.500e-02, ...,  3.700e-02,\n           9.000e-03, -7.700e-02],\n         [-9.860e-01, -1.310e-01,  7.700e-02, ...,  5.000e-03,\n          -1.000e-03, -7.700e-02],\n         ...,\n         [-9.880e-01, -1.250e-01,  8.300e-02, ..., -8.000e-03,\n           2.100e-02, -7.700e-02],\n         [-9.920e-01, -1.220e-01,  7.700e-02, ..., -1.400e-02,\n           1.200e-02, -7.700e-02],\n         [-9.960e-01, -1.290e-01,  7.500e-02, ..., -1.600e-02,\n          -1.500e-02, -7.700e-02]],\n \n        [[-9.810e-01, -1.190e-01,  8.700e-02, ..., -1.000e-03,\n           1.200e-02, -7.700e-02],\n         [-9.810e-01, -1.180e-01,  8.800e-02, ...,  0.000e+00,\n           1.200e-02, -7.700e-02],\n         [-9.850e-01, -1.160e-01,  8.700e-02, ...,  3.200e-02,\n           2.500e-02, -7.700e-02],\n         ...,\n         [-9.970e-01, -1.310e-01,  7.400e-02, ...,  1.200e-02,\n           3.400e-02, -7.700e-02],\n         [-9.940e-01, -1.150e-01,  7.300e-02, ..., -2.200e-02,\n           3.100e-02, -7.700e-02],\n         [-9.940e-01, -1.360e-01,  7.700e-02, ..., -1.500e-02,\n           6.000e-03, -7.700e-02]],\n \n        [[-9.960e-01, -1.290e-01,  7.500e-02, ..., -1.600e-02,\n          -1.500e-02, -7.700e-02],\n         [-9.930e-01, -1.250e-01,  7.500e-02, ...,  3.000e-03,\n          -7.000e-03, -7.700e-02],\n         [-9.910e-01, -1.270e-01,  7.400e-02, ...,  3.000e-03,\n           1.100e-02, -7.700e-02],\n         ...,\n         [-9.890e-01, -1.280e-01,  8.800e-02, ...,  1.000e-03,\n          -2.600e-02, -7.700e-02],\n         [-9.840e-01, -1.420e-01,  8.600e-02, ..., -1.500e-02,\n           3.500e-02, -7.700e-02],\n         [-9.850e-01, -1.660e-01,  8.600e-02, ...,  0.000e+00,\n           2.000e-03, -7.700e-02]]]),\n array([1., 1., 1., ..., 1., 1., 1.]),\n array([[[-9.620e-01, -1.660e-01,  9.600e-02, ...,  0.000e+00,\n           3.000e-03,  1.390e-01],\n         [-9.610e-01, -1.550e-01,  7.900e-02, ...,  6.000e-03,\n           3.900e-02,  1.400e-01],\n         [-9.810e-01, -1.510e-01,  6.900e-02, ...,  5.000e-03,\n           3.700e-02,  1.400e-01],\n         ...,\n         [-9.900e-01, -1.730e-01,  3.600e-02, ...,  1.700e-02,\n          -3.500e-02,  1.390e-01],\n         [-9.630e-01, -1.540e-01,  2.700e-02, ..., -7.000e-03,\n           4.300e-02,  1.390e-01],\n         [-9.710e-01, -1.500e-01,  2.700e-02, ..., -2.300e-02,\n           1.300e-02,  1.400e-01]],\n \n        [[-9.900e-01, -1.530e-01,  5.700e-02, ...,  9.000e-03,\n           1.600e-02,  1.400e-01],\n         [-9.850e-01, -1.750e-01,  6.900e-02, ..., -2.000e-02,\n           2.600e-02,  1.390e-01],\n         [-9.890e-01, -1.960e-01,  1.000e-02, ..., -4.400e-02,\n           5.000e-03,  1.380e-01],\n         ...,\n         [-1.003e+00, -1.680e-01,  1.600e-02, ...,  2.500e-02,\n           3.700e-02,  1.410e-01],\n         [-9.940e-01, -1.800e-01,  6.000e-03, ..., -9.000e-03,\n           1.600e-02,  1.410e-01],\n         [-9.890e-01, -1.800e-01,  7.000e-03, ...,  1.000e-03,\n           3.000e-03,  1.400e-01]],\n \n        [[-9.710e-01, -1.500e-01,  2.700e-02, ..., -2.300e-02,\n           1.300e-02,  1.400e-01],\n         [-9.760e-01, -1.510e-01,  4.000e-02, ..., -2.000e-03,\n          -8.000e-03,  1.400e-01],\n         [-9.920e-01, -1.450e-01,  3.000e-02, ...,  1.200e-02,\n           3.000e-02,  1.390e-01],\n         ...,\n         [-9.900e-01, -1.610e-01,  1.500e-02, ...,  7.000e-03,\n           0.000e+00,  1.400e-01],\n         [-9.850e-01, -1.560e-01,  6.000e-03, ...,  1.500e-02,\n          -3.500e-02,  1.400e-01],\n         [-9.810e-01, -1.610e-01,  1.000e-03, ..., -1.600e-02,\n           5.000e-03,  1.400e-01]],\n \n        ...,\n \n        [[-9.690e-01, -4.600e-02,  2.530e-01, ..., -2.900e-02,\n          -1.500e-02,  1.200e-02],\n         [-9.660e-01, -4.300e-02,  2.530e-01, ...,  5.200e-02,\n           2.200e-02,  1.200e-02],\n         [-9.710e-01, -4.500e-02,  2.500e-01, ...,  1.700e-02,\n           0.000e+00,  1.200e-02],\n         ...,\n         [-9.720e-01, -3.600e-02,  2.480e-01, ...,  9.000e-03,\n           2.300e-02,  1.200e-02],\n         [-9.690e-01, -4.300e-02,  2.620e-01, ...,  1.100e-02,\n           3.500e-02,  1.200e-02],\n         [-9.660e-01, -3.600e-02,  2.500e-01, ..., -1.500e-02,\n          -2.000e-03,  1.200e-02]],\n \n        [[-9.670e-01, -3.900e-02,  2.490e-01, ..., -1.800e-02,\n           1.100e-02,  1.200e-02],\n         [-9.690e-01, -4.100e-02,  2.530e-01, ..., -1.200e-02,\n          -2.800e-02,  1.200e-02],\n         [-9.700e-01, -4.000e-02,  2.540e-01, ..., -1.200e-02,\n          -1.200e-02,  1.200e-02],\n         ...,\n         [-9.710e-01, -5.000e-02,  2.510e-01, ...,  2.800e-02,\n          -1.700e-02,  1.200e-02],\n         [-9.690e-01, -5.000e-02,  2.500e-01, ..., -8.000e-03,\n          -6.000e-02,  1.200e-02],\n         [-9.670e-01, -4.900e-02,  2.470e-01, ...,  2.800e-02,\n          -1.400e-02,  1.200e-02]],\n \n        [[ 0.000e+00,  0.000e+00,  0.000e+00, ...,  0.000e+00,\n           0.000e+00,  0.000e+00],\n         [ 0.000e+00,  0.000e+00,  0.000e+00, ...,  0.000e+00,\n           0.000e+00,  0.000e+00],\n         [ 0.000e+00,  0.000e+00,  0.000e+00, ...,  0.000e+00,\n           0.000e+00,  0.000e+00],\n         ...,\n         [ 0.000e+00,  0.000e+00,  0.000e+00, ...,  0.000e+00,\n           0.000e+00,  0.000e+00],\n         [ 0.000e+00,  0.000e+00,  0.000e+00, ...,  0.000e+00,\n           0.000e+00,  0.000e+00],\n         [ 0.000e+00,  0.000e+00,  0.000e+00, ...,  0.000e+00,\n           0.000e+00,  0.000e+00]]]),\n array([1., 1., 1., ..., 1., 1., 0.]))"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_obj = Dataset(\"pa2\")\n",
    "dataset_obj.read_dataset()\n",
    "dataset_obj.downsample_dataset()\n",
    "dataset_obj.segment_dataset()\n",
    "\n",
    "dataset_obj2 = Dataset(\"opp\")\n",
    "dataset_obj2.read_dataset()\n",
    "dataset_obj2.downsample_dataset()\n",
    "dataset_obj2.segment_dataset()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Train on pretext task: flip transformation"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "opp\n",
      "X shape = (?, 1, 23, 77)\n",
      "Y shape = (?, 2)\n",
      "WARNING:tensorflow:From /home/dmitrii/anaconda3/envs/work/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py:201: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "hidden layer 1 shape\n",
      "W_conv1 shape = (1, 7, 77, 128)\n",
      "b_conv1 shape = (128,)\n",
      "h_conv1 shape = (?, 1, 17, 128)\n",
      "h_pool1 shape = (?, 1, 6, 128)\n",
      "hidden layer 2 shape\n",
      "W_conv2 shape = (1, 3, 128, 128)\n",
      "b_conv2 shape = (128,)\n",
      "h_conv2 shape = (?, 1, 4, 128)\n",
      "h_pool2 shape = (?, 1, 1, 128)\n",
      "shape's shape: [None, 1, 1, 128]\n",
      "c_flat shape = (?, 128)\n",
      "unique test_y [0 1]\n",
      "unique train_y [0 1]\n",
      "test_y[1]= [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "train_y shape(1-hot) = (63650, 18)\n",
      "test_y shape(1-hot) = (10955, 18)\n",
      "train_x_reshaped =  (63650, 1, 23, 77)\n",
      "test_x_reshaped =  (10955, 1, 23, 77)\n",
      "train_x shape = (63650, 23, 77)\n",
      "train_y shape = (63650,)\n",
      "test_x shape = (10955, 23, 77)\n",
      "test_y shape = (10955,)\n",
      "WARNING:tensorflow:From /home/dmitrii/anaconda3/envs/work/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py:201: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n",
      "WARNING:tensorflow:From /home/dmitrii/anaconda3/envs/work/lib/python3.8/site-packages/tensorflow/python/util/tf_should_use.py:247: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-10 12:31:30.746269: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2022-07-10 12:31:30.746434: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-07-10 12:31:30.748311: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n",
      "2022-07-10 12:31:30.753389: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:196] None of the MLIR optimization passes are enabled (registered 0 passes)\n",
      "2022-07-10 12:31:30.777108: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 1796665000 Hz\n",
      "/home/dmitrii/GitHub/SDAPraktikum/E3/transformation.py:24: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  splitted = np.array(np.split(sample_i, np.append(segments, sample.shape[1])))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0  Training Loss:  0.9206255541690879  Training Accuracy:  0.704839\n",
      "Epoch:  1  Training Loss:  0.42423838230800465  Training Accuracy:  0.8040377\n",
      "Epoch:  2  Training Loss:  0.30825699688833647  Training Accuracy:  0.84494895\n",
      "Epoch:  3  Training Loss:  0.2485126395346281  Training Accuracy:  0.8716261\n",
      "Epoch:  4  Training Loss:  0.20981551894930367  Training Accuracy:  0.89762765\n",
      "Epoch:  5  Training Loss:  0.19119743491147376  Training Accuracy:  0.9087353\n",
      "Epoch:  6  Training Loss:  0.17678990340181538  Training Accuracy:  0.90785545\n",
      "Epoch:  7  Training Loss:  0.1702188974557905  Training Accuracy:  0.9347525\n",
      "Epoch:  8  Training Loss:  0.14819459372406313  Training Accuracy:  0.94061273\n",
      "Epoch:  9  Training Loss:  0.13133881309607806  Training Accuracy:  0.9296465\n",
      "Epoch:  10  Training Loss:  0.13127729413480352  Training Accuracy:  0.94637865\n",
      "Epoch:  11  Training Loss:  0.1228902751171521  Training Accuracy:  0.9447447\n",
      "Epoch:  12  Training Loss:  0.13888512538014477  Training Accuracy:  0.9476512\n",
      "Epoch:  13  Training Loss:  0.1255337949478493  Training Accuracy:  0.9436606\n",
      "Epoch:  14  Training Loss:  0.10687032504067148  Training Accuracy:  0.9504792\n",
      "Epoch:  15  Training Loss:  0.11634083612965843  Training Accuracy:  0.9643676\n",
      "Epoch:  16  Training Loss:  0.11060346312080105  Training Accuracy:  0.9523645\n",
      "Epoch:  17  Training Loss:  0.08577101740903277  Training Accuracy:  0.9635507\n",
      "Epoch:  18  Training Loss:  0.10064819960219924  Training Accuracy:  0.95861745\n",
      "Epoch:  19  Training Loss:  0.10835096482034336  Training Accuracy:  0.94307935\n",
      "Epoch:  20  Training Loss:  0.08467833718212149  Training Accuracy:  0.94757265\n",
      "Epoch:  21  Training Loss:  0.08260385108307008  Training Accuracy:  0.97000784\n",
      "Epoch:  22  Training Loss:  0.09841531233009372  Training Accuracy:  0.970055\n",
      "Epoch:  23  Training Loss:  0.0867039802120327  Training Accuracy:  0.96521604\n",
      "Epoch:  24  Training Loss:  0.08681983071805899  Training Accuracy:  0.95327574\n",
      "Epoch:  25  Training Loss:  0.07068051469289728  Training Accuracy:  0.96769834\n",
      "Epoch:  26  Training Loss:  0.08521853899445545  Training Accuracy:  0.9606756\n",
      "Epoch:  27  Training Loss:  0.08748532371472813  Training Accuracy:  0.9697722\n",
      "Epoch:  28  Training Loss:  0.08121311441187958  Training Accuracy:  0.9709505\n",
      "Epoch:  29  Training Loss:  0.07277378888190628  Training Accuracy:  0.9790102\n",
      "Epoch:  30  Training Loss:  0.05533956857870044  Training Accuracy:  0.9847604\n",
      "Epoch:  31  Training Loss:  0.0678431450497853  Training Accuracy:  0.9602514\n",
      "Epoch:  32  Training Loss:  0.0882330150532048  Training Accuracy:  0.95564806\n",
      "Epoch:  33  Training Loss:  0.06459038513004983  Training Accuracy:  0.96884525\n",
      "Epoch:  34  Training Loss:  0.055071492943425934  Training Accuracy:  0.9842734\n",
      "Epoch:  35  Training Loss:  0.0627382675502739  Training Accuracy:  0.9703221\n",
      "Epoch:  36  Training Loss:  0.07798347845683361  Training Accuracy:  0.96037704\n",
      "Epoch:  37  Training Loss:  0.05369513972794941  Training Accuracy:  0.9591516\n",
      "Epoch:  38  Training Loss:  0.05758430809790443  Training Accuracy:  0.97673213\n",
      "Epoch:  39  Training Loss:  0.07560715962138728  Training Accuracy:  0.9801728\n",
      "Epoch:  40  Training Loss:  0.0689275943740028  Training Accuracy:  0.98237234\n",
      "Epoch:  41  Training Loss:  0.04948796526247153  Training Accuracy:  0.96798116\n",
      "Epoch:  42  Training Loss:  0.04501752108047378  Training Accuracy:  0.9881854\n",
      "Epoch:  43  Training Loss:  0.03719355434270853  Training Accuracy:  0.9890809\n",
      "Epoch:  44  Training Loss:  0.05671819294828361  Training Accuracy:  0.9596858\n",
      "Epoch:  45  Training Loss:  0.042220988644191285  Training Accuracy:  0.98783976\n",
      "Epoch:  46  Training Loss:  0.060001444517924225  Training Accuracy:  0.98639435\n",
      "Epoch:  47  Training Loss:  0.041284586337835875  Training Accuracy:  0.9752553\n",
      "Epoch:  48  Training Loss:  0.04390331600757527  Training Accuracy:  0.9710448\n",
      "Epoch:  49  Training Loss:  0.060144494794018524  Training Accuracy:  0.9820267\n"
     ]
    }
   ],
   "source": [
    "model2 = Model(dataset_obj2)\n",
    "model2.build_model(self_training=True, num_transforms=2)\n",
    "transforms = [identity, permute]\n",
    "model2.self_train(transforms, 50)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "model = Model(dataset_obj)\n",
    "stats = Statistics()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pa2\n",
      "X shape = (?, 1, 25, 52)\n",
      "Y shape = (?, 11)\n",
      "hidden layer 1 shape\n",
      "W_conv1 shape = (1, 7, 52, 128)\n",
      "b_conv1 shape = (128,)\n",
      "h_conv1 shape = (?, 1, 19, 128)\n",
      "h_pool1 shape = (?, 1, 7, 128)\n",
      "WEIGHTS ARE LOADED\n",
      "WEIGHTS ARE LOADED\n",
      "hidden layer 2 shape\n",
      "W_conv2 shape = (1, 3, 128, 128)\n",
      "b_conv2 shape = (128,)\n",
      "h_conv2 shape = (?, 1, 5, 128)\n",
      "h_pool2 shape = (?, 1, 2, 128)\n",
      "shape's shape: [None, 1, 2, 128]\n",
      "c_flat shape = (?, 256)\n",
      "unique test_y [0 1]\n",
      "unique train_y [0 1]\n",
      "test_y[1]= [1 0 0 0 0 0 0 0 0 0 0]\n",
      "train_y shape(1-hot) = (3561, 11)\n",
      "test_y shape(1-hot) = (628, 11)\n",
      "train_x_reshaped =  (3561, 1, 25, 52)\n",
      "test_x_reshaped =  (628, 1, 25, 52)\n",
      "train_x shape = (3561, 25, 52)\n",
      "train_y shape = (3561,)\n",
      "test_x shape = (628, 25, 52)\n",
      "test_y shape = (628,)\n",
      "Epoch:  0  Training Loss:  2.4443649226968938  Training Accuracy:  0.11625948\n",
      "Epoch:  1  Training Loss:  2.339885856888511  Training Accuracy:  0.16512215\n",
      "Epoch:  2  Training Loss:  2.2830466335470025  Training Accuracy:  0.19545072\n",
      "Epoch:  3  Training Loss:  2.2359164649789984  Training Accuracy:  0.23083404\n",
      "Epoch:  4  Training Loss:  2.173899711262096  Training Accuracy:  0.28306654\n",
      "Epoch:  5  Training Loss:  2.115669436888261  Training Accuracy:  0.33782646\n",
      "Epoch:  6  Training Loss:  2.0594240275296296  Training Accuracy:  0.35158664\n",
      "Epoch:  7  Training Loss:  1.9932833043011753  Training Accuracy:  0.38668913\n",
      "Epoch:  8  Training Loss:  1.9314113302664324  Training Accuracy:  0.393148\n",
      "Epoch:  9  Training Loss:  1.8762094454331832  Training Accuracy:  0.41533276\n",
      "Epoch:  10  Training Loss:  1.811306829886003  Training Accuracy:  0.41898343\n",
      "Epoch:  11  Training Loss:  1.7722683397206394  Training Accuracy:  0.43442854\n",
      "Epoch:  12  Training Loss:  1.7151103182272478  Training Accuracy:  0.45689413\n",
      "Epoch:  13  Training Loss:  1.6604347705841065  Training Accuracy:  0.46391463\n",
      "Epoch:  14  Training Loss:  1.6173098786310716  Training Accuracy:  0.47346252\n",
      "Epoch:  15  Training Loss:  1.5814419415864078  Training Accuracy:  0.5009829\n",
      "Epoch:  16  Training Loss:  1.518036112460223  Training Accuracy:  0.50772256\n",
      "Epoch:  17  Training Loss:  1.499907983974977  Training Accuracy:  0.51418144\n",
      "Epoch:  18  Training Loss:  1.4476367078044199  Training Accuracy:  0.5369278\n",
      "Epoch:  19  Training Loss:  1.4359881596131758  Training Accuracy:  0.5391744\n",
      "Epoch:  20  Training Loss:  1.392881443825635  Training Accuracy:  0.5563044\n",
      "Epoch:  21  Training Loss:  1.3703761956908487  Training Accuracy:  0.5661331\n",
      "Epoch:  22  Training Loss:  1.3386083808812228  Training Accuracy:  0.5754002\n",
      "Epoch:  23  Training Loss:  1.3169607249173252  Training Accuracy:  0.5703454\n",
      "Epoch:  24  Training Loss:  1.2832775728269057  Training Accuracy:  0.6023589\n",
      "Epoch:  25  Training Loss:  1.2567312966693531  Training Accuracy:  0.6017972\n",
      "Epoch:  26  Training Loss:  1.2314474013718693  Training Accuracy:  0.6093794\n",
      "Epoch:  27  Training Loss:  1.2088028252124787  Training Accuracy:  0.61050266\n",
      "Epoch:  28  Training Loss:  1.1875127339904958  Training Accuracy:  0.62454367\n",
      "Epoch:  29  Training Loss:  1.156262903321873  Training Accuracy:  0.6265094\n",
      "Epoch:  30  Training Loss:  1.1350949905135415  Training Accuracy:  0.638023\n",
      "Epoch:  31  Training Loss:  1.11279560435902  Training Accuracy:  0.64672846\n",
      "Epoch:  32  Training Loss:  1.0966206409714438  Training Accuracy:  0.6559955\n",
      "Epoch:  33  Training Loss:  1.0688281958753412  Training Accuracy:  0.6573996\n",
      "Epoch:  34  Training Loss:  1.0502591260454872  Training Accuracy:  0.66217357\n",
      "Epoch:  35  Training Loss:  1.0385568970983678  Training Accuracy:  0.6666667\n",
      "Epoch:  36  Training Loss:  1.0114545339887793  Training Accuracy:  0.6759337\n",
      "Epoch:  37  Training Loss:  0.999677743694999  Training Accuracy:  0.6837967\n",
      "Epoch:  38  Training Loss:  0.9791748523712158  Training Accuracy:  0.68800896\n",
      "Epoch:  39  Training Loss:  0.9556196161291816  Training Accuracy:  0.6936254\n",
      "Epoch:  40  Training Loss:  0.9520365484736183  Training Accuracy:  0.6981185\n",
      "Epoch:  41  Training Loss:  0.9226874898780476  Training Accuracy:  0.7028924\n",
      "Epoch:  42  Training Loss:  0.9228562845425172  Training Accuracy:  0.7026116\n",
      "Epoch:  43  Training Loss:  0.90523649697954  Training Accuracy:  0.7138444\n",
      "Epoch:  44  Training Loss:  0.8844829952174967  Training Accuracy:  0.7144061\n",
      "Epoch:  45  Training Loss:  0.8765933402559974  Training Accuracy:  0.7247964\n",
      "Epoch:  46  Training Loss:  0.8504601031541824  Training Accuracy:  0.7247964\n",
      "Epoch:  47  Training Loss:  0.8379829910668459  Training Accuracy:  0.73069364\n",
      "Epoch:  48  Training Loss:  0.8406290203332901  Training Accuracy:  0.744173\n",
      "Epoch:  49  Training Loss:  0.8228990072553808  Training Accuracy:  0.7405223\n",
      "Epoch:  50  Training Loss:  0.7979711351069537  Training Accuracy:  0.74501544\n",
      "Epoch:  51  Training Loss:  0.7827740525657481  Training Accuracy:  0.75428253\n",
      "Epoch:  52  Training Loss:  0.7750385763970289  Training Accuracy:  0.7483853\n",
      "Epoch:  53  Training Loss:  0.7676632344722748  Training Accuracy:  0.7627071\n",
      "Epoch:  54  Training Loss:  0.7516987415877255  Training Accuracy:  0.7677619\n",
      "Epoch:  55  Training Loss:  0.7364156151359732  Training Accuracy:  0.767481\n",
      "Epoch:  56  Training Loss:  0.7232956970279867  Training Accuracy:  0.7708509\n",
      "Epoch:  57  Training Loss:  0.7177462312308225  Training Accuracy:  0.76691943\n",
      "Epoch:  58  Training Loss:  0.7084345665845004  Training Accuracy:  0.769166\n",
      "Epoch:  59  Training Loss:  0.6915186280553991  Training Accuracy:  0.7801179\n",
      "Epoch:  60  Training Loss:  0.6903166264295578  Training Accuracy:  0.7806796\n",
      "Epoch:  61  Training Loss:  0.6745127956975591  Training Accuracy:  0.7913507\n",
      "Epoch:  62  Training Loss:  0.6540655152364211  Training Accuracy:  0.7933165\n",
      "Epoch:  63  Training Loss:  0.6531219027259133  Training Accuracy:  0.80174106\n",
      "Epoch:  64  Training Loss:  0.635697337036783  Training Accuracy:  0.80117947\n",
      "Epoch:  65  Training Loss:  0.6242025709965012  Training Accuracy:  0.8056726\n",
      "Epoch:  66  Training Loss:  0.6197935042056171  Training Accuracy:  0.80960405\n",
      "Epoch:  67  Training Loss:  0.6107282419096339  Training Accuracy:  0.8107273\n",
      "Epoch:  68  Training Loss:  0.5973222637718374  Training Accuracy:  0.81381637\n",
      "Epoch:  69  Training Loss:  0.590357066961852  Training Accuracy:  0.81241226\n",
      "Epoch:  70  Training Loss:  0.5909424430944703  Training Accuracy:  0.8155013\n",
      "Epoch:  71  Training Loss:  0.5766398112882267  Training Accuracy:  0.8121314\n",
      "Epoch:  72  Training Loss:  0.5658977702260017  Training Accuracy:  0.8233642\n",
      "Epoch:  73  Training Loss:  0.5641666229475628  Training Accuracy:  0.83347374\n",
      "Epoch:  74  Training Loss:  0.5459301402623004  Training Accuracy:  0.8312272\n",
      "Epoch:  75  Training Loss:  0.5410248664292422  Training Accuracy:  0.83375454\n",
      "Epoch:  76  Training Loss:  0.5292833107438955  Training Accuracy:  0.8258916\n",
      "Epoch:  77  Training Loss:  0.5265708343549208  Training Accuracy:  0.83656275\n",
      "Epoch:  78  Training Loss:  0.5122005908326669  Training Accuracy:  0.83347374\n",
      "Epoch:  79  Training Loss:  0.496570264886726  Training Accuracy:  0.84246\n",
      "Epoch:  80  Training Loss:  0.5037323065779425  Training Accuracy:  0.8435833\n",
      "Epoch:  81  Training Loss:  0.4891356499357657  Training Accuracy:  0.83824766\n",
      "Epoch:  82  Training Loss:  0.48896465545350853  Training Accuracy:  0.8357203\n",
      "Epoch:  83  Training Loss:  0.4745906033299186  Training Accuracy:  0.8407751\n",
      "Epoch:  84  Training Loss:  0.473522411422296  Training Accuracy:  0.8418983\n",
      "Epoch:  85  Training Loss:  0.45657648986036126  Training Accuracy:  0.8494805\n",
      "Epoch:  86  Training Loss:  0.45242539928718045  Training Accuracy:  0.8466723\n",
      "Epoch:  87  Training Loss:  0.4410915393720974  Training Accuracy:  0.85228866\n",
      "Epoch:  88  Training Loss:  0.44892366589470345  Training Accuracy:  0.8494805\n",
      "Epoch:  89  Training Loss:  0.4397004666653546  Training Accuracy:  0.856501\n",
      "Epoch:  90  Training Loss:  0.4219098940491676  Training Accuracy:  0.85734344\n",
      "Epoch:  91  Training Loss:  0.42229774946516213  Training Accuracy:  0.8511654\n",
      "Epoch:  92  Training Loss:  0.4161813918839801  Training Accuracy:  0.8539736\n",
      "Epoch:  93  Training Loss:  0.42410808571360326  Training Accuracy:  0.8548161\n",
      "Epoch:  94  Training Loss:  0.4051928000016646  Training Accuracy:  0.85734344\n",
      "Epoch:  95  Training Loss:  0.41023952283642506  Training Accuracy:  0.85088456\n",
      "Epoch:  96  Training Loss:  0.40557392144745047  Training Accuracy:  0.85987085\n",
      "Epoch:  97  Training Loss:  0.39791211661967363  Training Accuracy:  0.8581859\n",
      "Epoch:  98  Training Loss:  0.38643034466288306  Training Accuracy:  0.86436397\n",
      "Epoch:  99  Training Loss:  0.3886860394342379  Training Accuracy:  0.8621174\n",
      "INFO:tensorflow:Restoring parameters from pa2\n",
      "Testing Accuracy: 0.89171976\n",
      "validation accuracy: 0.89171976\n",
      "pa2\n",
      "X shape = (?, 1, 25, 52)\n",
      "Y shape = (?, 11)\n",
      "hidden layer 1 shape\n",
      "W_conv1 shape = (1, 7, 52, 128)\n",
      "b_conv1 shape = (128,)\n",
      "h_conv1 shape = (?, 1, 19, 128)\n",
      "h_pool1 shape = (?, 1, 7, 128)\n",
      "WEIGHTS ARE LOADED\n",
      "WEIGHTS ARE LOADED\n",
      "hidden layer 2 shape\n",
      "W_conv2 shape = (1, 3, 128, 128)\n",
      "b_conv2 shape = (128,)\n",
      "h_conv2 shape = (?, 1, 5, 128)\n",
      "h_pool2 shape = (?, 1, 2, 128)\n",
      "shape's shape: [None, 1, 2, 128]\n",
      "c_flat shape = (?, 256)\n",
      "unique test_y [0 1]\n",
      "unique train_y [0 1]\n",
      "test_y[1]= [1 0 0 0 0 0 0 0 0 0 0]\n",
      "train_y shape(1-hot) = (3561, 11)\n",
      "test_y shape(1-hot) = (628, 11)\n",
      "train_x_reshaped =  (3561, 1, 25, 52)\n",
      "test_x_reshaped =  (628, 1, 25, 52)\n",
      "train_x shape = (3561, 25, 52)\n",
      "train_y shape = (3561,)\n",
      "test_x shape = (628, 25, 52)\n",
      "test_y shape = (628,)\n",
      "Epoch:  0  Training Loss:  2.4539348884062333  Training Accuracy:  0.1024993\n",
      "Epoch:  1  Training Loss:  2.354723637754267  Training Accuracy:  0.14911541\n",
      "Epoch:  2  Training Loss:  2.308710882880471  Training Accuracy:  0.20668352\n",
      "Epoch:  3  Training Loss:  2.2545487078753386  Training Accuracy:  0.23897782\n",
      "Epoch:  4  Training Loss:  2.198114837299694  Training Accuracy:  0.2614434\n",
      "Epoch:  5  Training Loss:  2.140119125626304  Training Accuracy:  0.2920528\n",
      "Epoch:  6  Training Loss:  2.0859623085368764  Training Accuracy:  0.3105869\n",
      "Epoch:  7  Training Loss:  2.0314012386582116  Training Accuracy:  0.326032\n",
      "Epoch:  8  Training Loss:  1.9818107355724681  Training Accuracy:  0.36169615\n",
      "Epoch:  9  Training Loss:  1.927850942178206  Training Accuracy:  0.368155\n",
      "Epoch:  10  Training Loss:  1.881745207309723  Training Accuracy:  0.4057849\n",
      "Epoch:  11  Training Loss:  1.8227496098388325  Training Accuracy:  0.41673687\n",
      "Epoch:  12  Training Loss:  1.7832625231959602  Training Accuracy:  0.42937377\n",
      "Epoch:  13  Training Loss:  1.7328339151360772  Training Accuracy:  0.44987363\n",
      "Epoch:  14  Training Loss:  1.6876010743054477  Training Accuracy:  0.4689694\n",
      "Epoch:  15  Training Loss:  1.6473129267042332  Training Accuracy:  0.48497614\n",
      "Epoch:  16  Training Loss:  1.5925500373948704  Training Accuracy:  0.49648976\n",
      "Epoch:  17  Training Loss:  1.5506753374229778  Training Accuracy:  0.50772256\n",
      "Epoch:  18  Training Loss:  1.5114187213507566  Training Accuracy:  0.5251334\n",
      "Epoch:  19  Training Loss:  1.4772569743069734  Training Accuracy:  0.53159225\n",
      "Epoch:  20  Training Loss:  1.4430529123002833  Training Accuracy:  0.5504072\n",
      "Epoch:  21  Training Loss:  1.3985011740164324  Training Accuracy:  0.5532154\n",
      "Epoch:  22  Training Loss:  1.3679590143940665  Training Accuracy:  0.5675372\n",
      "Epoch:  23  Training Loss:  1.3439354647289623  Training Accuracy:  0.57259196\n",
      "Epoch:  24  Training Loss:  1.3161614217541435  Training Accuracy:  0.5891603\n",
      "Epoch:  25  Training Loss:  1.2842257911508734  Training Accuracy:  0.59196854\n",
      "Epoch:  26  Training Loss:  1.2652668784965169  Training Accuracy:  0.59646165\n",
      "Epoch:  27  Training Loss:  1.2369490265846252  Training Accuracy:  0.5998315\n",
      "Epoch:  28  Training Loss:  1.2184007525444032  Training Accuracy:  0.6093794\n",
      "Epoch:  29  Training Loss:  1.1965489371256395  Training Accuracy:  0.63240665\n",
      "Epoch:  30  Training Loss:  1.1675672953779048  Training Accuracy:  0.6206122\n",
      "Epoch:  31  Training Loss:  1.1537698057564822  Training Accuracy:  0.63605726\n",
      "Epoch:  32  Training Loss:  1.1397949522191828  Training Accuracy:  0.6332491\n",
      "Epoch:  33  Training Loss:  1.1109064370393753  Training Accuracy:  0.6363381\n",
      "Epoch:  34  Training Loss:  1.0830508318814365  Training Accuracy:  0.66076946\n",
      "Epoch:  35  Training Loss:  1.0677594379945234  Training Accuracy:  0.6573996\n",
      "Epoch:  36  Training Loss:  1.0626464794982564  Training Accuracy:  0.6663858\n",
      "Epoch:  37  Training Loss:  1.0441143621097912  Training Accuracy:  0.6529065\n",
      "Epoch:  38  Training Loss:  1.035545386509462  Training Accuracy:  0.6647009\n",
      "Epoch:  39  Training Loss:  1.018581355159933  Training Accuracy:  0.67481047\n",
      "Epoch:  40  Training Loss:  1.00318875096061  Training Accuracy:  0.67200226\n",
      "Epoch:  41  Training Loss:  0.9856384190646085  Training Accuracy:  0.6759337\n",
      "Epoch:  42  Training Loss:  0.974128642949191  Training Accuracy:  0.6919405\n",
      "Epoch:  43  Training Loss:  0.9499675886197524  Training Accuracy:  0.6933446\n",
      "Epoch:  44  Training Loss:  0.9505063755945726  Training Accuracy:  0.68800896\n",
      "Epoch:  45  Training Loss:  0.9305884599685669  Training Accuracy:  0.70036507\n",
      "Epoch:  46  Training Loss:  0.913538991321217  Training Accuracy:  0.6950295\n",
      "Epoch:  47  Training Loss:  0.9081404247067192  Training Accuracy:  0.7042965\n",
      "Epoch:  48  Training Loss:  0.890842613848773  Training Accuracy:  0.7174951\n",
      "Epoch:  49  Training Loss:  0.8704262551936236  Training Accuracy:  0.72002244\n",
      "Epoch:  50  Training Loss:  0.8661875061013482  Training Accuracy:  0.71637183\n",
      "Epoch:  51  Training Loss:  0.8533350215716795  Training Accuracy:  0.72002244\n",
      "Epoch:  52  Training Loss:  0.8418249550190839  Training Accuracy:  0.7262005\n",
      "Epoch:  53  Training Loss:  0.8366908802227541  Training Accuracy:  0.73715246\n",
      "Epoch:  54  Training Loss:  0.8207112840630791  Training Accuracy:  0.7349059\n",
      "Epoch:  55  Training Loss:  0.8016454767097126  Training Accuracy:  0.7424881\n",
      "Epoch:  56  Training Loss:  0.7992170935327356  Training Accuracy:  0.7396799\n",
      "Epoch:  57  Training Loss:  0.7810658991336823  Training Accuracy:  0.74333054\n",
      "Epoch:  58  Training Loss:  0.7758523624051701  Training Accuracy:  0.7455771\n",
      "Epoch:  59  Training Loss:  0.7714964584870772  Training Accuracy:  0.75905645\n",
      "Epoch:  60  Training Loss:  0.7480621289123188  Training Accuracy:  0.7556866\n",
      "Epoch:  61  Training Loss:  0.743277092684399  Training Accuracy:  0.75765234\n",
      "Epoch:  62  Training Loss:  0.7295923682776364  Training Accuracy:  0.7598989\n",
      "Epoch:  63  Training Loss:  0.7253089885820042  Training Accuracy:  0.76860434\n",
      "Epoch:  64  Training Loss:  0.7087082226168026  Training Accuracy:  0.77057004\n",
      "Epoch:  65  Training Loss:  0.709252788532864  Training Accuracy:  0.7694468\n",
      "Epoch:  66  Training Loss:  0.6864495261148973  Training Accuracy:  0.77197415\n",
      "Epoch:  67  Training Loss:  0.6831160884011875  Training Accuracy:  0.77646726\n",
      "Epoch:  68  Training Loss:  0.6703630490736527  Training Accuracy:  0.77197415\n",
      "Epoch:  69  Training Loss:  0.6674227308143269  Training Accuracy:  0.78264534\n",
      "Epoch:  70  Training Loss:  0.6562965924089605  Training Accuracy:  0.78854257\n",
      "Epoch:  71  Training Loss:  0.63241190422665  Training Accuracy:  0.78292614\n",
      "Epoch:  72  Training Loss:  0.6389822374690662  Training Accuracy:  0.7899466\n",
      "Epoch:  73  Training Loss:  0.6355435570532625  Training Accuracy:  0.7927548\n",
      "Epoch:  74  Training Loss:  0.6253841077739543  Training Accuracy:  0.78770006\n",
      "Epoch:  75  Training Loss:  0.6177911785515872  Training Accuracy:  0.79696715\n",
      "Epoch:  76  Training Loss:  0.6026527135209604  Training Accuracy:  0.8037068\n",
      "Epoch:  77  Training Loss:  0.5998908709396016  Training Accuracy:  0.8006178\n",
      "Epoch:  78  Training Loss:  0.5941237785599448  Training Accuracy:  0.803426\n",
      "Epoch:  79  Training Loss:  0.5866731493310495  Training Accuracy:  0.8031452\n",
      "Epoch:  80  Training Loss:  0.5790946801955049  Training Accuracy:  0.80819994\n",
      "Epoch:  81  Training Loss:  0.5690100198442286  Training Accuracy:  0.81381637\n",
      "Epoch:  82  Training Loss:  0.5573621060360562  Training Accuracy:  0.81269306\n",
      "Epoch:  83  Training Loss:  0.5585424851287495  Training Accuracy:  0.8115698\n",
      "Epoch:  84  Training Loss:  0.5472652879628268  Training Accuracy:  0.8101657\n",
      "Epoch:  85  Training Loss:  0.5327480428598144  Training Accuracy:  0.8275765\n",
      "Epoch:  86  Training Loss:  0.5264410800554535  Training Accuracy:  0.82055604\n",
      "Epoch:  87  Training Loss:  0.5164683598009023  Training Accuracy:  0.8228026\n",
      "Epoch:  88  Training Loss:  0.5088930453766476  Training Accuracy:  0.82111764\n",
      "Epoch:  89  Training Loss:  0.5117248310284181  Training Accuracy:  0.8242067\n",
      "Epoch:  90  Training Loss:  0.4965472657572139  Training Accuracy:  0.83319294\n",
      "Epoch:  91  Training Loss:  0.5022606558420442  Training Accuracy:  0.8309464\n",
      "Epoch:  92  Training Loss:  0.4961573527617888  Training Accuracy:  0.83515865\n",
      "Epoch:  93  Training Loss:  0.48175780150023373  Training Accuracy:  0.8407751\n",
      "Epoch:  94  Training Loss:  0.47290019609711387  Training Accuracy:  0.8402134\n",
      "Epoch:  95  Training Loss:  0.47403596300970424  Training Accuracy:  0.8385285\n",
      "Epoch:  96  Training Loss:  0.46892796836116096  Training Accuracy:  0.8326313\n",
      "Epoch:  97  Training Loss:  0.463281452384862  Training Accuracy:  0.83628196\n",
      "Epoch:  98  Training Loss:  0.46058331687342036  Training Accuracy:  0.84582984\n",
      "Epoch:  99  Training Loss:  0.45434446118094707  Training Accuracy:  0.8438641\n",
      "INFO:tensorflow:Restoring parameters from pa2\n",
      "Testing Accuracy: 0.8757962\n",
      "validation accuracy: 0.8757962\n",
      "pa2\n",
      "X shape = (?, 1, 25, 52)\n",
      "Y shape = (?, 11)\n",
      "hidden layer 1 shape\n",
      "W_conv1 shape = (1, 7, 52, 128)\n",
      "b_conv1 shape = (128,)\n",
      "h_conv1 shape = (?, 1, 19, 128)\n",
      "h_pool1 shape = (?, 1, 7, 128)\n",
      "WEIGHTS ARE LOADED\n",
      "WEIGHTS ARE LOADED\n",
      "hidden layer 2 shape\n",
      "W_conv2 shape = (1, 3, 128, 128)\n",
      "b_conv2 shape = (128,)\n",
      "h_conv2 shape = (?, 1, 5, 128)\n",
      "h_pool2 shape = (?, 1, 2, 128)\n",
      "shape's shape: [None, 1, 2, 128]\n",
      "c_flat shape = (?, 256)\n",
      "unique test_y [0 1]\n",
      "unique train_y [0 1]\n",
      "test_y[1]= [1 0 0 0 0 0 0 0 0 0 0]\n",
      "train_y shape(1-hot) = (3561, 11)\n",
      "test_y shape(1-hot) = (628, 11)\n",
      "train_x_reshaped =  (3561, 1, 25, 52)\n",
      "test_x_reshaped =  (628, 1, 25, 52)\n",
      "train_x shape = (3561, 25, 52)\n",
      "train_y shape = (3561,)\n",
      "test_x shape = (628, 25, 52)\n",
      "test_y shape = (628,)\n",
      "Epoch:  0  Training Loss:  2.4359152598814533  Training Accuracy:  0.11541702\n",
      "Epoch:  1  Training Loss:  2.3279786153273148  Training Accuracy:  0.16119067\n",
      "Epoch:  2  Training Loss:  2.2906802437522193  Training Accuracy:  0.21454647\n",
      "Epoch:  3  Training Loss:  2.231576505574313  Training Accuracy:  0.25863522\n",
      "Epoch:  4  Training Loss:  2.1868111176924274  Training Accuracy:  0.29345688\n",
      "Epoch:  5  Training Loss:  2.1387493762102996  Training Accuracy:  0.31648412\n",
      "Epoch:  6  Training Loss:  2.0808905493129384  Training Accuracy:  0.31788823\n",
      "Epoch:  7  Training Loss:  2.0267430316318165  Training Accuracy:  0.33754563\n",
      "Epoch:  8  Training Loss:  1.9581393469463695  Training Accuracy:  0.35214826\n",
      "Epoch:  9  Training Loss:  1.9202847610820424  Training Accuracy:  0.35664138\n",
      "Epoch:  10  Training Loss:  1.8823919556357644  Training Accuracy:  0.36141533\n",
      "Epoch:  11  Training Loss:  1.8355493767694993  Training Accuracy:  0.37742206\n",
      "Epoch:  12  Training Loss:  1.7926423186605627  Training Accuracy:  0.40409997\n",
      "Epoch:  13  Training Loss:  1.7532379551367325  Training Accuracy:  0.41027802\n",
      "Epoch:  14  Training Loss:  1.7196579174561935  Training Accuracy:  0.41589442\n",
      "Epoch:  15  Training Loss:  1.6910494262521918  Training Accuracy:  0.43948328\n",
      "Epoch:  16  Training Loss:  1.6582864132794466  Training Accuracy:  0.4605448\n",
      "Epoch:  17  Training Loss:  1.6290495612404563  Training Accuracy:  0.48076382\n",
      "Epoch:  18  Training Loss:  1.5893235423348167  Training Accuracy:  0.4928391\n",
      "Epoch:  19  Training Loss:  1.5617951626127415  Training Accuracy:  0.51867455\n",
      "Epoch:  20  Training Loss:  1.5223380278457295  Training Accuracy:  0.5307498\n",
      "Epoch:  21  Training Loss:  1.4937718125906858  Training Accuracy:  0.5386127\n",
      "Epoch:  22  Training Loss:  1.465078009258617  Training Accuracy:  0.5481606\n",
      "Epoch:  23  Training Loss:  1.4299493060870605  Training Accuracy:  0.5593934\n",
      "Epoch:  24  Training Loss:  1.4008826144716957  Training Accuracy:  0.5731536\n",
      "Epoch:  25  Training Loss:  1.3723460674285888  Training Accuracy:  0.5827015\n",
      "Epoch:  26  Training Loss:  1.3524917353283274  Training Accuracy:  0.59926987\n",
      "Epoch:  27  Training Loss:  1.3110685608603738  Training Accuracy:  0.5987082\n",
      "Epoch:  28  Training Loss:  1.295518063957041  Training Accuracy:  0.60909855\n",
      "Epoch:  29  Training Loss:  1.2645781473679976  Training Accuracy:  0.63156414\n",
      "Epoch:  30  Training Loss:  1.2455615108663385  Training Accuracy:  0.62482446\n",
      "Epoch:  31  Training Loss:  1.2110404778610577  Training Accuracy:  0.62959844\n",
      "Epoch:  32  Training Loss:  1.1840319926088507  Training Accuracy:  0.64504355\n",
      "Epoch:  33  Training Loss:  1.163874009793455  Training Accuracy:  0.6517832\n",
      "Epoch:  34  Training Loss:  1.1532054752111436  Training Accuracy:  0.6526257\n",
      "Epoch:  35  Training Loss:  1.1333469168706374  Training Accuracy:  0.6604886\n",
      "Epoch:  36  Training Loss:  1.1081262826919556  Training Accuracy:  0.65824205\n",
      "Epoch:  37  Training Loss:  1.0857094249942085  Training Accuracy:  0.6602078\n",
      "Epoch:  38  Training Loss:  1.062758534604853  Training Accuracy:  0.67087895\n",
      "Epoch:  39  Training Loss:  1.0451099509542638  Training Accuracy:  0.6812693\n",
      "Epoch:  40  Training Loss:  1.0322234308177776  Training Accuracy:  0.6807077\n",
      "Epoch:  41  Training Loss:  1.0098733005198566  Training Accuracy:  0.68744737\n",
      "Epoch:  42  Training Loss:  1.0116187079386278  Training Accuracy:  0.6902555\n",
      "Epoch:  43  Training Loss:  0.9838772334835746  Training Accuracy:  0.6882898\n",
      "Epoch:  44  Training Loss:  0.9616587706587532  Training Accuracy:  0.69278294\n",
      "Epoch:  45  Training Loss:  0.9534897806969556  Training Accuracy:  0.7028924\n",
      "Epoch:  46  Training Loss:  0.9451028897003694  Training Accuracy:  0.7096321\n",
      "Epoch:  47  Training Loss:  0.9331111008470708  Training Accuracy:  0.7158102\n",
      "Epoch:  48  Training Loss:  0.9125995725393296  Training Accuracy:  0.71918\n",
      "Epoch:  49  Training Loss:  0.8933005224574696  Training Accuracy:  0.716091\n",
      "Epoch:  50  Training Loss:  0.8888535561886701  Training Accuracy:  0.7233923\n",
      "Epoch:  51  Training Loss:  0.8693353455175054  Training Accuracy:  0.7219882\n",
      "Epoch:  52  Training Loss:  0.8647980711676858  Training Accuracy:  0.73546755\n",
      "Epoch:  53  Training Loss:  0.8603639163754203  Training Accuracy:  0.73350185\n",
      "Epoch:  54  Training Loss:  0.8353684102947062  Training Accuracy:  0.7455771\n",
      "Epoch:  55  Training Loss:  0.8353965268893675  Training Accuracy:  0.7349059\n",
      "Epoch:  56  Training Loss:  0.8154581741853194  Training Accuracy:  0.7424881\n",
      "Epoch:  57  Training Loss:  0.8032012622464787  Training Accuracy:  0.7475428\n",
      "Epoch:  58  Training Loss:  0.7883375379172238  Training Accuracy:  0.75456333\n",
      "Epoch:  59  Training Loss:  0.7893416464328766  Training Accuracy:  0.7556866\n",
      "Epoch:  60  Training Loss:  0.78036763261665  Training Accuracy:  0.7483853\n",
      "Epoch:  61  Training Loss:  0.7618892825462601  Training Accuracy:  0.76074135\n",
      "Epoch:  62  Training Loss:  0.7507877645167438  Training Accuracy:  0.7736591\n",
      "Epoch:  63  Training Loss:  0.7372062918814746  Training Accuracy:  0.76383036\n",
      "Epoch:  64  Training Loss:  0.7253940037705682  Training Accuracy:  0.7725358\n",
      "Epoch:  65  Training Loss:  0.7236964055083015  Training Accuracy:  0.772255\n",
      "Epoch:  66  Training Loss:  0.7067876947197047  Training Accuracy:  0.78292614\n",
      "Epoch:  67  Training Loss:  0.7025449269197204  Training Accuracy:  0.78152204\n",
      "Epoch:  68  Training Loss:  0.691394780576229  Training Accuracy:  0.7775906\n",
      "Epoch:  69  Training Loss:  0.6793484606526115  Training Accuracy:  0.78741926\n",
      "Epoch:  70  Training Loss:  0.6747592438351024  Training Accuracy:  0.7879809\n",
      "Epoch:  71  Training Loss:  0.6564431218938394  Training Accuracy:  0.79247403\n",
      "Epoch:  72  Training Loss:  0.6579433453353969  Training Accuracy:  0.7944398\n",
      "Epoch:  73  Training Loss:  0.6400996481830423  Training Accuracy:  0.7935973\n",
      "Epoch:  74  Training Loss:  0.6302924592386593  Training Accuracy:  0.8008986\n",
      "Epoch:  75  Training Loss:  0.6311172931031748  Training Accuracy:  0.8037068\n",
      "Epoch:  76  Training Loss:  0.6141654387116432  Training Accuracy:  0.79977536\n",
      "Epoch:  77  Training Loss:  0.6168389986861836  Training Accuracy:  0.81409717\n",
      "Epoch:  78  Training Loss:  0.6000500745394013  Training Accuracy:  0.81381637\n",
      "Epoch:  79  Training Loss:  0.5989482779394496  Training Accuracy:  0.80988485\n",
      "Epoch:  80  Training Loss:  0.5978605942969972  Training Accuracy:  0.8107273\n",
      "Epoch:  81  Training Loss:  0.5713079991665754  Training Accuracy:  0.8171862\n",
      "Epoch:  82  Training Loss:  0.5708523026921533  Training Accuracy:  0.8216793\n",
      "Epoch:  83  Training Loss:  0.5560773758725687  Training Accuracy:  0.8171862\n",
      "Epoch:  84  Training Loss:  0.5523126710544933  Training Accuracy:  0.8233642\n",
      "Epoch:  85  Training Loss:  0.548698275197636  Training Accuracy:  0.8213985\n",
      "Epoch:  86  Training Loss:  0.5416876070201397  Training Accuracy:  0.8247683\n",
      "Epoch:  87  Training Loss:  0.5399353086948395  Training Accuracy:  0.81943274\n",
      "Epoch:  88  Training Loss:  0.5312891685149886  Training Accuracy:  0.82645327\n",
      "Epoch:  89  Training Loss:  0.5240909619764849  Training Accuracy:  0.83656275\n",
      "Epoch:  90  Training Loss:  0.5269847462123091  Training Accuracy:  0.83375454\n",
      "Epoch:  91  Training Loss:  0.5091786917637695  Training Accuracy:  0.8309464\n",
      "Epoch:  92  Training Loss:  0.5089400283314965  Training Accuracy:  0.8354395\n",
      "Epoch:  93  Training Loss:  0.49417485188354143  Training Accuracy:  0.8306655\n",
      "Epoch:  94  Training Loss:  0.493132305551659  Training Accuracy:  0.84302163\n",
      "Epoch:  95  Training Loss:  0.481218833476305  Training Accuracy:  0.83824766\n",
      "Epoch:  96  Training Loss:  0.4827829249203205  Training Accuracy:  0.8413367\n",
      "Epoch:  97  Training Loss:  0.4725941026752645  Training Accuracy:  0.8449874\n",
      "Epoch:  98  Training Loss:  0.4711570104414767  Training Accuracy:  0.8433024\n",
      "Epoch:  99  Training Loss:  0.4574521561915224  Training Accuracy:  0.85032296\n",
      "INFO:tensorflow:Restoring parameters from pa2\n",
      "Testing Accuracy: 0.88535035\n",
      "validation accuracy: 0.88535035\n",
      "pa2\n",
      "X shape = (?, 1, 25, 52)\n",
      "Y shape = (?, 11)\n",
      "hidden layer 1 shape\n",
      "W_conv1 shape = (1, 7, 52, 128)\n",
      "b_conv1 shape = (128,)\n",
      "h_conv1 shape = (?, 1, 19, 128)\n",
      "h_pool1 shape = (?, 1, 7, 128)\n",
      "WEIGHTS ARE LOADED\n",
      "WEIGHTS ARE LOADED\n",
      "hidden layer 2 shape\n",
      "W_conv2 shape = (1, 3, 128, 128)\n",
      "b_conv2 shape = (128,)\n",
      "h_conv2 shape = (?, 1, 5, 128)\n",
      "h_pool2 shape = (?, 1, 2, 128)\n",
      "shape's shape: [None, 1, 2, 128]\n",
      "c_flat shape = (?, 256)\n",
      "unique test_y [0 1]\n",
      "unique train_y [0 1]\n",
      "test_y[1]= [1 0 0 0 0 0 0 0 0 0 0]\n",
      "train_y shape(1-hot) = (3561, 11)\n",
      "test_y shape(1-hot) = (628, 11)\n",
      "train_x_reshaped =  (3561, 1, 25, 52)\n",
      "test_x_reshaped =  (628, 1, 25, 52)\n",
      "train_x shape = (3561, 25, 52)\n",
      "train_y shape = (3561,)\n",
      "test_x shape = (628, 25, 52)\n",
      "test_y shape = (628,)\n",
      "Epoch:  0  Training Loss:  2.423586071621288  Training Accuracy:  0.11064308\n",
      "Epoch:  1  Training Loss:  2.3456831151788884  Training Accuracy:  0.13872507\n",
      "Epoch:  2  Training Loss:  2.3051335378126665  Training Accuracy:  0.17719741\n",
      "Epoch:  3  Training Loss:  2.255921686779369  Training Accuracy:  0.21791631\n",
      "Epoch:  4  Training Loss:  2.2033214590766215  Training Accuracy:  0.25049144\n",
      "Epoch:  5  Training Loss:  2.14755032712763  Training Accuracy:  0.27688852\n",
      "Epoch:  6  Training Loss:  2.088757954944264  Training Accuracy:  0.30553216\n",
      "Epoch:  7  Training Loss:  2.032545629414645  Training Accuracy:  0.33529907\n",
      "Epoch:  8  Training Loss:  1.982565165649761  Training Accuracy:  0.3442853\n",
      "Epoch:  9  Training Loss:  1.9401348114013672  Training Accuracy:  0.36703172\n",
      "Epoch:  10  Training Loss:  1.89141315872019  Training Accuracy:  0.38528502\n",
      "Epoch:  11  Training Loss:  1.8411269973624835  Training Accuracy:  0.4116821\n",
      "Epoch:  12  Training Loss:  1.8053039073944093  Training Accuracy:  0.41982588\n",
      "Epoch:  13  Training Loss:  1.7569270643320951  Training Accuracy:  0.44172984\n",
      "Epoch:  14  Training Loss:  1.718848653273149  Training Accuracy:  0.45268184\n",
      "Epoch:  15  Training Loss:  1.6812628052451395  Training Accuracy:  0.46812692\n",
      "Epoch:  16  Training Loss:  1.6417109695347873  Training Accuracy:  0.47795564\n",
      "Epoch:  17  Training Loss:  1.6103181714361363  Training Accuracy:  0.49340072\n",
      "Epoch:  18  Training Loss:  1.5697606493126262  Training Accuracy:  0.50519514\n",
      "Epoch:  19  Training Loss:  1.5378983679142866  Training Accuracy:  0.52176356\n",
      "Epoch:  20  Training Loss:  1.5055493609471755  Training Accuracy:  0.53973603\n",
      "Epoch:  21  Training Loss:  1.4692941080440174  Training Accuracy:  0.54001683\n",
      "Epoch:  22  Training Loss:  1.4434939763762735  Training Accuracy:  0.5529346\n",
      "Epoch:  23  Training Loss:  1.4033321391452442  Training Accuracy:  0.5655715\n",
      "Epoch:  24  Training Loss:  1.3683076132427563  Training Accuracy:  0.5728728\n",
      "Epoch:  25  Training Loss:  1.3435739788142118  Training Accuracy:  0.5891603\n",
      "Epoch:  26  Training Loss:  1.3089308817278256  Training Accuracy:  0.5871946\n",
      "Epoch:  27  Training Loss:  1.2709043995900589  Training Accuracy:  0.6020781\n",
      "Epoch:  28  Training Loss:  1.2557987055995248  Training Accuracy:  0.59505755\n",
      "Epoch:  29  Training Loss:  1.2258845681493933  Training Accuracy:  0.6138725\n",
      "Epoch:  30  Training Loss:  1.210147759589282  Training Accuracy:  0.6206122\n",
      "Epoch:  31  Training Loss:  1.1698342550884593  Training Accuracy:  0.628756\n",
      "Epoch:  32  Training Loss:  1.1519211500883102  Training Accuracy:  0.6338107\n",
      "Epoch:  33  Training Loss:  1.1303950130939484  Training Accuracy:  0.6515024\n",
      "Epoch:  34  Training Loss:  1.1090202510356904  Training Accuracy:  0.6472901\n",
      "Epoch:  35  Training Loss:  1.0941080765290694  Training Accuracy:  0.6573996\n",
      "Epoch:  36  Training Loss:  1.0616682784123854  Training Accuracy:  0.67087895\n",
      "Epoch:  37  Training Loss:  1.044054266539487  Training Accuracy:  0.6663858\n",
      "Epoch:  38  Training Loss:  1.0116391062736512  Training Accuracy:  0.6742488\n",
      "Epoch:  39  Training Loss:  1.010612227158113  Training Accuracy:  0.67874193\n",
      "Epoch:  40  Training Loss:  0.9881904767318206  Training Accuracy:  0.6899747\n",
      "Epoch:  41  Training Loss:  0.978401295434345  Training Accuracy:  0.6888515\n",
      "Epoch:  42  Training Loss:  0.9473043739795685  Training Accuracy:  0.69755685\n",
      "Epoch:  43  Training Loss:  0.9315885619683699  Training Accuracy:  0.7009267\n",
      "Epoch:  44  Training Loss:  0.9281576124104587  Training Accuracy:  0.70878965\n",
      "Epoch:  45  Training Loss:  0.9025587393478913  Training Accuracy:  0.70991296\n",
      "Epoch:  46  Training Loss:  0.8946090313521299  Training Accuracy:  0.71805674\n",
      "Epoch:  47  Training Loss:  0.877344467423179  Training Accuracy:  0.7169334\n",
      "Epoch:  48  Training Loss:  0.8643858259374445  Training Accuracy:  0.7247964\n",
      "Epoch:  49  Training Loss:  0.8389396637678146  Training Accuracy:  0.7262005\n",
      "Epoch:  50  Training Loss:  0.8430803588845514  Training Accuracy:  0.7312553\n",
      "Epoch:  51  Training Loss:  0.8127210099588741  Training Accuracy:  0.73855656\n",
      "Epoch:  52  Training Loss:  0.7989394252950495  Training Accuracy:  0.7422072\n",
      "Epoch:  53  Training Loss:  0.7919795144687999  Training Accuracy:  0.74501544\n",
      "Epoch:  54  Training Loss:  0.7790276928381487  Training Accuracy:  0.7556866\n",
      "Epoch:  55  Training Loss:  0.7710657382553274  Training Accuracy:  0.74810445\n",
      "Epoch:  56  Training Loss:  0.757609096440402  Training Accuracy:  0.75063187\n",
      "Epoch:  57  Training Loss:  0.7318789904767816  Training Accuracy:  0.7596181\n",
      "Epoch:  58  Training Loss:  0.7338159569285133  Training Accuracy:  0.7610222\n",
      "Epoch:  59  Training Loss:  0.7249082765795968  Training Accuracy:  0.7646728\n",
      "Epoch:  60  Training Loss:  0.7082005614584143  Training Accuracy:  0.76720023\n",
      "Epoch:  61  Training Loss:  0.7040352856571024  Training Accuracy:  0.7694468\n",
      "Epoch:  62  Training Loss:  0.6857536874034188  Training Accuracy:  0.77506316\n",
      "Epoch:  63  Training Loss:  0.67028893801299  Training Accuracy:  0.7775906\n",
      "Epoch:  64  Training Loss:  0.6650506108999252  Training Accuracy:  0.7899466\n",
      "Epoch:  65  Training Loss:  0.6524786396460099  Training Accuracy:  0.7834878\n",
      "Epoch:  66  Training Loss:  0.6459137732332403  Training Accuracy:  0.78713846\n",
      "Epoch:  67  Training Loss:  0.6334079552780498  Training Accuracy:  0.79556304\n",
      "Epoch:  68  Training Loss:  0.6284189709208229  Training Accuracy:  0.7896658\n",
      "Epoch:  69  Training Loss:  0.6173705430193381  Training Accuracy:  0.79696715\n",
      "Epoch:  70  Training Loss:  0.6131806276061318  Training Accuracy:  0.8006178\n",
      "Epoch:  71  Training Loss:  0.5968192929571325  Training Accuracy:  0.8020219\n",
      "Epoch:  72  Training Loss:  0.5894535663453015  Training Accuracy:  0.8101657\n",
      "Epoch:  73  Training Loss:  0.5923975922844626  Training Accuracy:  0.8166245\n",
      "Epoch:  74  Training Loss:  0.5727086069909009  Training Accuracy:  0.81297386\n",
      "Epoch:  75  Training Loss:  0.568299093300646  Training Accuracy:  0.8188711\n",
      "Epoch:  76  Training Loss:  0.547784856368195  Training Accuracy:  0.82252175\n",
      "Epoch:  77  Training Loss:  0.5404184659773653  Training Accuracy:  0.82252175\n",
      "Epoch:  78  Training Loss:  0.5340369599786672  Training Accuracy:  0.8199944\n",
      "Epoch:  79  Training Loss:  0.5392231530763886  Training Accuracy:  0.82392585\n",
      "Epoch:  80  Training Loss:  0.5196901515126229  Training Accuracy:  0.82111764\n",
      "Epoch:  81  Training Loss:  0.5185606506737795  Training Accuracy:  0.82224095\n",
      "Epoch:  82  Training Loss:  0.508588909696449  Training Accuracy:  0.82392585\n",
      "Epoch:  83  Training Loss:  0.5041487738490105  Training Accuracy:  0.83319294\n",
      "Epoch:  84  Training Loss:  0.49590581330386074  Training Accuracy:  0.83768606\n",
      "Epoch:  85  Training Loss:  0.4838121835481037  Training Accuracy:  0.83824766\n",
      "Epoch:  86  Training Loss:  0.46951303766532376  Training Accuracy:  0.8402134\n",
      "Epoch:  87  Training Loss:  0.4724684521555901  Training Accuracy:  0.83628196\n",
      "Epoch:  88  Training Loss:  0.47404533434997903  Training Accuracy:  0.8441449\n",
      "Epoch:  89  Training Loss:  0.46140344034541736  Training Accuracy:  0.8497613\n",
      "Epoch:  90  Training Loss:  0.4602887386625463  Training Accuracy:  0.84105587\n",
      "Epoch:  91  Training Loss:  0.44872420328584584  Training Accuracy:  0.8466723\n",
      "Epoch:  92  Training Loss:  0.4410438526760448  Training Accuracy:  0.8466723\n",
      "Epoch:  93  Training Loss:  0.44285828281532635  Training Accuracy:  0.8480764\n",
      "Epoch:  94  Training Loss:  0.4364128291606903  Training Accuracy:  0.85060376\n",
      "Epoch:  95  Training Loss:  0.4337614652108062  Training Accuracy:  0.84751475\n",
      "Epoch:  96  Training Loss:  0.4301879613914273  Training Accuracy:  0.848638\n",
      "Epoch:  97  Training Loss:  0.4171001840721477  Training Accuracy:  0.8500421\n",
      "Epoch:  98  Training Loss:  0.4133488256822933  Training Accuracy:  0.85565853\n",
      "Epoch:  99  Training Loss:  0.41365816742181777  Training Accuracy:  0.8567818\n",
      "INFO:tensorflow:Restoring parameters from pa2\n",
      "Testing Accuracy: 0.8933121\n",
      "validation accuracy: 0.8933121\n",
      "pa2\n",
      "X shape = (?, 1, 25, 52)\n",
      "Y shape = (?, 11)\n",
      "hidden layer 1 shape\n",
      "W_conv1 shape = (1, 7, 52, 128)\n",
      "b_conv1 shape = (128,)\n",
      "h_conv1 shape = (?, 1, 19, 128)\n",
      "h_pool1 shape = (?, 1, 7, 128)\n",
      "WEIGHTS ARE LOADED\n",
      "WEIGHTS ARE LOADED\n",
      "hidden layer 2 shape\n",
      "W_conv2 shape = (1, 3, 128, 128)\n",
      "b_conv2 shape = (128,)\n",
      "h_conv2 shape = (?, 1, 5, 128)\n",
      "h_pool2 shape = (?, 1, 2, 128)\n",
      "shape's shape: [None, 1, 2, 128]\n",
      "c_flat shape = (?, 256)\n",
      "unique test_y [0 1]\n",
      "unique train_y [0 1]\n",
      "test_y[1]= [1 0 0 0 0 0 0 0 0 0 0]\n",
      "train_y shape(1-hot) = (3561, 11)\n",
      "test_y shape(1-hot) = (628, 11)\n",
      "train_x_reshaped =  (3561, 1, 25, 52)\n",
      "test_x_reshaped =  (628, 1, 25, 52)\n",
      "train_x shape = (3561, 25, 52)\n",
      "train_y shape = (3561,)\n",
      "test_x shape = (628, 25, 52)\n",
      "test_y shape = (628,)\n",
      "Epoch:  0  Training Loss:  2.4685229128057307  Training Accuracy:  0.110362254\n",
      "Epoch:  1  Training Loss:  2.3448807564648715  Training Accuracy:  0.15332772\n",
      "Epoch:  2  Training Loss:  2.2934811808846214  Training Accuracy:  0.1881494\n",
      "Epoch:  3  Training Loss:  2.2439335584640503  Training Accuracy:  0.22381353\n",
      "Epoch:  4  Training Loss:  2.1839777794751254  Training Accuracy:  0.27042964\n",
      "Epoch:  5  Training Loss:  2.136020120707425  Training Accuracy:  0.27913508\n",
      "Epoch:  6  Training Loss:  2.0838370355692777  Training Accuracy:  0.30440888\n",
      "Epoch:  7  Training Loss:  2.037932580167597  Training Accuracy:  0.31873068\n",
      "Epoch:  8  Training Loss:  1.9747019030831077  Training Accuracy:  0.3358607\n",
      "Epoch:  9  Training Loss:  1.931097117879174  Training Accuracy:  0.34709352\n",
      "Epoch:  10  Training Loss:  1.888032287901098  Training Accuracy:  0.36225778\n",
      "Epoch:  11  Training Loss:  1.8618552077900279  Training Accuracy:  0.37461388\n",
      "Epoch:  12  Training Loss:  1.810496557300741  Training Accuracy:  0.3864083\n",
      "Epoch:  13  Training Loss:  1.7754307931119746  Training Accuracy:  0.39399046\n",
      "Epoch:  14  Training Loss:  1.7458051177588376  Training Accuracy:  0.41757932\n",
      "Epoch:  15  Training Loss:  1.722830694913864  Training Accuracy:  0.43246278\n",
      "Epoch:  16  Training Loss:  1.6830670272762125  Training Accuracy:  0.45212018\n",
      "Epoch:  17  Training Loss:  1.6614958486773752  Training Accuracy:  0.46531874\n",
      "Epoch:  18  Training Loss:  1.6246439126404848  Training Accuracy:  0.48778433\n",
      "Epoch:  19  Training Loss:  1.5930170351808721  Training Accuracy:  0.49255827\n",
      "Epoch:  20  Training Loss:  1.5679043734615499  Training Accuracy:  0.5096883\n",
      "Epoch:  21  Training Loss:  1.5360546380281448  Training Accuracy:  0.5153047\n",
      "Epoch:  22  Training Loss:  1.5063316789540377  Training Accuracy:  0.5461949\n",
      "Epoch:  23  Training Loss:  1.4674507477066734  Training Accuracy:  0.54900306\n",
      "Epoch:  24  Training Loss:  1.4454869077964263  Training Accuracy:  0.55686605\n",
      "Epoch:  25  Training Loss:  1.4309664173559709  Training Accuracy:  0.5633249\n",
      "Epoch:  26  Training Loss:  1.39613300670277  Training Accuracy:  0.58073574\n",
      "Epoch:  27  Training Loss:  1.3541526583108034  Training Accuracy:  0.58691376\n",
      "Epoch:  28  Training Loss:  1.331042843786153  Training Accuracy:  0.5953384\n",
      "Epoch:  29  Training Loss:  1.3091180056333542  Training Accuracy:  0.6032013\n",
      "Epoch:  30  Training Loss:  1.2751038085330617  Training Accuracy:  0.6141533\n",
      "Epoch:  31  Training Loss:  1.2495638801292939  Training Accuracy:  0.620893\n",
      "Epoch:  32  Training Loss:  1.234881655736403  Training Accuracy:  0.62145466\n",
      "Epoch:  33  Training Loss:  1.2051708736202933  Training Accuracy:  0.6346532\n",
      "Epoch:  34  Training Loss:  1.1830359849062833  Training Accuracy:  0.628756\n",
      "Epoch:  35  Training Loss:  1.159262560714375  Training Accuracy:  0.64700925\n",
      "Epoch:  36  Training Loss:  1.1339731614698063  Training Accuracy:  0.6554339\n",
      "Epoch:  37  Training Loss:  1.1198310245167125  Training Accuracy:  0.6464476\n",
      "Epoch:  38  Training Loss:  1.1158454022624276  Training Accuracy:  0.67059815\n",
      "Epoch:  39  Training Loss:  1.0707358793778854  Training Accuracy:  0.6697557\n",
      "Epoch:  40  Training Loss:  1.0556847566908056  Training Accuracy:  0.6714406\n",
      "Epoch:  41  Training Loss:  1.034927487373352  Training Accuracy:  0.67396796\n",
      "Epoch:  42  Training Loss:  1.025061722235246  Training Accuracy:  0.67509127\n",
      "Epoch:  43  Training Loss:  1.003224509412592  Training Accuracy:  0.6888515\n",
      "Epoch:  44  Training Loss:  0.9848987690427087  Training Accuracy:  0.6933446\n",
      "Epoch:  45  Training Loss:  0.9752179714766416  Training Accuracy:  0.69615275\n",
      "Epoch:  46  Training Loss:  0.9575026566332037  Training Accuracy:  0.70850885\n",
      "Epoch:  47  Training Loss:  0.9470281386917287  Training Accuracy:  0.70991296\n",
      "Epoch:  48  Training Loss:  0.9237974240021272  Training Accuracy:  0.7152485\n",
      "Epoch:  49  Training Loss:  0.9090784555131739  Training Accuracy:  0.7264813\n",
      "Epoch:  50  Training Loss:  0.8997318573973395  Training Accuracy:  0.72451556\n",
      "Epoch:  51  Training Loss:  0.8822794491594488  Training Accuracy:  0.72114575\n",
      "Epoch:  52  Training Loss:  0.8686032628471201  Training Accuracy:  0.7304128\n",
      "Epoch:  53  Training Loss:  0.8537139716473493  Training Accuracy:  0.73406345\n",
      "Epoch:  54  Training Loss:  0.8444666962731968  Training Accuracy:  0.74192643\n",
      "Epoch:  55  Training Loss:  0.8352425702593543  Training Accuracy:  0.73715246\n",
      "Epoch:  56  Training Loss:  0.8161232479593971  Training Accuracy:  0.7430497\n",
      "Epoch:  57  Training Loss:  0.8017843574285507  Training Accuracy:  0.7469812\n",
      "Epoch:  58  Training Loss:  0.7873720553788272  Training Accuracy:  0.75119346\n",
      "Epoch:  59  Training Loss:  0.7711285840381276  Training Accuracy:  0.7531592\n",
      "Epoch:  60  Training Loss:  0.7697984513911333  Training Accuracy:  0.7598989\n",
      "Epoch:  61  Training Loss:  0.7520428795706142  Training Accuracy:  0.7598989\n",
      "Epoch:  62  Training Loss:  0.7455266500061208  Training Accuracy:  0.76691943\n",
      "Epoch:  63  Training Loss:  0.7353834049268202  Training Accuracy:  0.77028924\n",
      "Epoch:  64  Training Loss:  0.7195171689445322  Training Accuracy:  0.7739399\n",
      "Epoch:  65  Training Loss:  0.7107005990364335  Training Accuracy:  0.7728166\n",
      "Epoch:  66  Training Loss:  0.6904210835695267  Training Accuracy:  0.772255\n",
      "Epoch:  67  Training Loss:  0.6951010796156797  Training Accuracy:  0.7851727\n",
      "Epoch:  68  Training Loss:  0.6685176955028014  Training Accuracy:  0.78713846\n",
      "Epoch:  69  Training Loss:  0.6740287301215259  Training Accuracy:  0.7910699\n",
      "Epoch:  70  Training Loss:  0.6545893701640042  Training Accuracy:  0.7905083\n",
      "Epoch:  71  Training Loss:  0.6420860474759882  Training Accuracy:  0.80174106\n",
      "Epoch:  72  Training Loss:  0.6413762821392579  Training Accuracy:  0.79837126\n",
      "Epoch:  73  Training Loss:  0.6242030384865674  Training Accuracy:  0.8107273\n",
      "Epoch:  74  Training Loss:  0.6129240493882786  Training Accuracy:  0.8025835\n",
      "Epoch:  75  Training Loss:  0.5970458844845945  Training Accuracy:  0.80005616\n",
      "Epoch:  76  Training Loss:  0.5940969998186285  Training Accuracy:  0.8087616\n",
      "Epoch:  77  Training Loss:  0.5913469046354294  Training Accuracy:  0.8073575\n",
      "Epoch:  78  Training Loss:  0.5714132926680825  Training Accuracy:  0.8188711\n",
      "Epoch:  79  Training Loss:  0.5739151106639342  Training Accuracy:  0.8135355\n",
      "Epoch:  80  Training Loss:  0.5587365050207485  Training Accuracy:  0.8163437\n",
      "Epoch:  81  Training Loss:  0.5582442600618709  Training Accuracy:  0.8216793\n",
      "Epoch:  82  Training Loss:  0.5361786017363722  Training Accuracy:  0.82645327\n",
      "Epoch:  83  Training Loss:  0.5348029282960025  Training Accuracy:  0.8354395\n",
      "Epoch:  84  Training Loss:  0.5340515945445408  Training Accuracy:  0.8233642\n",
      "Epoch:  85  Training Loss:  0.52783935286782  Training Accuracy:  0.8326313\n",
      "Epoch:  86  Training Loss:  0.5132111010226337  Training Accuracy:  0.8368436\n",
      "Epoch:  87  Training Loss:  0.5103748906742442  Training Accuracy:  0.83628196\n",
      "Epoch:  88  Training Loss:  0.5045570641756058  Training Accuracy:  0.8421792\n",
      "Epoch:  89  Training Loss:  0.49553785852410576  Training Accuracy:  0.83796686\n",
      "Epoch:  90  Training Loss:  0.48911490792577916  Training Accuracy:  0.8357203\n",
      "Epoch:  91  Training Loss:  0.4792464185844768  Training Accuracy:  0.83375454\n",
      "Epoch:  92  Training Loss:  0.467811866375533  Training Accuracy:  0.8399326\n",
      "Epoch:  93  Training Loss:  0.47160116745667024  Training Accuracy:  0.848638\n",
      "Epoch:  94  Training Loss:  0.4592329128222032  Training Accuracy:  0.8435833\n",
      "Epoch:  95  Training Loss:  0.45646411275321785  Training Accuracy:  0.8418983\n",
      "Epoch:  96  Training Loss:  0.44796847972002896  Training Accuracy:  0.84723395\n",
      "Epoch:  97  Training Loss:  0.4500078218227083  Training Accuracy:  0.8494805\n",
      "Epoch:  98  Training Loss:  0.4445145008916205  Training Accuracy:  0.8542544\n",
      "Epoch:  99  Training Loss:  0.43849950859492476  Training Accuracy:  0.85369277\n",
      "INFO:tensorflow:Restoring parameters from pa2\n",
      "Testing Accuracy: 0.9012739\n",
      "validation accuracy: 0.9012739\n"
     ]
    }
   ],
   "source": [
    "num_experiments = 5\n",
    "\n",
    "for i in range(num_experiments):\n",
    "    model.W_conv2_weight = model2.W_conv2_weight\n",
    "    model.b_conv2_weight = model2.b_conv2_weight\n",
    "    model.build_model(self_training=False, load_weights=True)\n",
    "    model.run_model(training_epochs=100)\n",
    "    test_accuracy, val_accuracy, y_pred, y_true = model.test()\n",
    "    stats.add(test_accuracy, val_accuracy, y_pred, y_true)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy. Mean (std) = 0.889 (0.009)\n",
      "Val accuracy. Mean (std) = 0.889 (0.009)\n",
      "f1 score weighted. Mean (std) = 0.887 (0.009)\n",
      "f1 score mean. Mean (std) = 0.870 (0.013)\n"
     ]
    }
   ],
   "source": [
    "stats.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Multi-class classification: flip, permute, time warp"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "opp\n",
      "X shape = (?, 1, 23, 77)\n",
      "Y shape = (?, 4)\n",
      "hidden layer 1 shape\n",
      "W_conv1 shape = (1, 7, 77, 128)\n",
      "b_conv1 shape = (128,)\n",
      "h_conv1 shape = (?, 1, 17, 128)\n",
      "h_pool1 shape = (?, 1, 6, 128)\n",
      "hidden layer 2 shape\n",
      "W_conv2 shape = (1, 3, 128, 128)\n",
      "b_conv2 shape = (128,)\n",
      "h_conv2 shape = (?, 1, 4, 128)\n",
      "h_pool2 shape = (?, 1, 1, 128)\n",
      "shape's shape: [None, 1, 1, 128]\n",
      "c_flat shape = (?, 128)\n",
      "unique test_y [0 1]\n",
      "unique train_y [0 1]\n",
      "test_y[1]= [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "train_y shape(1-hot) = (63650, 18)\n",
      "test_y shape(1-hot) = (10955, 18)\n",
      "train_x_reshaped =  (63650, 1, 23, 77)\n",
      "test_x_reshaped =  (10955, 1, 23, 77)\n",
      "train_x shape = (63650, 23, 77)\n",
      "train_y shape = (63650,)\n",
      "test_x shape = (10955, 23, 77)\n",
      "test_y shape = (10955,)\n",
      "Epoch:  0  Training Loss:  1.9894732367584642  Training Accuracy:  0.25391987\n",
      "Epoch:  1  Training Loss:  1.4137594233336344  Training Accuracy:  0.25118616\n",
      "Epoch:  2  Training Loss:  1.3979916831617145  Training Accuracy:  0.25085625\n",
      "Epoch:  3  Training Loss:  1.3957268167549455  Training Accuracy:  0.2518303\n",
      "Epoch:  4  Training Loss:  1.3925166017332786  Training Accuracy:  0.25044775\n",
      "Epoch:  5  Training Loss:  1.3918920014225982  Training Accuracy:  0.25219166\n",
      "Epoch:  6  Training Loss:  1.3915070914166794  Training Accuracy:  0.2515318\n",
      "Epoch:  7  Training Loss:  1.3908705966813224  Training Accuracy:  0.2507934\n",
      "Epoch:  8  Training Loss:  1.3898396683890575  Training Accuracy:  0.2591045\n",
      "Epoch:  9  Training Loss:  1.389136584232031  Training Accuracy:  0.25347996\n",
      "Epoch:  10  Training Loss:  1.3879308422326562  Training Accuracy:  0.25699922\n",
      "Epoch:  11  Training Loss:  1.3867899479280774  Training Accuracy:  0.2552396\n",
      "Epoch:  12  Training Loss:  1.3849825217450409  Training Accuracy:  0.2571563\n",
      "Epoch:  13  Training Loss:  1.383690291487235  Training Accuracy:  0.26465043\n",
      "Epoch:  14  Training Loss:  1.3812888595659727  Training Accuracy:  0.2650589\n",
      "Epoch:  15  Training Loss:  1.379541254259451  Training Accuracy:  0.2679183\n",
      "Epoch:  16  Training Loss:  1.3782900293588158  Training Accuracy:  0.2677769\n",
      "Epoch:  17  Training Loss:  1.3749604943532578  Training Accuracy:  0.26926944\n",
      "Epoch:  18  Training Loss:  1.3736317892909289  Training Accuracy:  0.2723959\n",
      "Epoch:  19  Training Loss:  1.3704489176901773  Training Accuracy:  0.27186173\n",
      "Epoch:  20  Training Loss:  1.3678974606381575  Training Accuracy:  0.27874312\n",
      "Epoch:  21  Training Loss:  1.3640467583293645  Training Accuracy:  0.27758053\n",
      "Epoch:  22  Training Loss:  1.3626994255562903  Training Accuracy:  0.27737626\n",
      "Epoch:  23  Training Loss:  1.360580700024273  Training Accuracy:  0.28413197\n",
      "Epoch:  24  Training Loss:  1.3580914577487968  Training Accuracy:  0.28268656\n",
      "Epoch:  25  Training Loss:  1.3547688662646042  Training Accuracy:  0.2845719\n",
      "Epoch:  26  Training Loss:  1.3523853151371301  Training Accuracy:  0.28571877\n"
     ]
    }
   ],
   "source": [
    "stats3 = Statistics()\n",
    "model3 = Model(dataset_obj2)\n",
    "model3.build_model(self_training=True, num_transforms=4)\n",
    "transforms = [identity, flip, permute, time_warp]\n",
    "model3.self_train(transforms, 50)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for i in range(num_experiments):\n",
    "    model.W_conv2_weight = model3.W_conv2_weight\n",
    "    model.b_conv2_weight = model3.b_conv2_weight\n",
    "    model.build_model(self_training=False, load_weights=True)\n",
    "    model.run_model(training_epochs=100)\n",
    "    test_accuracy, val_accuracy, y_pred, y_true = model.test()\n",
    "    stats3.add(test_accuracy, val_accuracy, y_pred, y_true)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "stats3.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}