{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7afe9f49",
   "metadata": {
    "id": "7afe9f49",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1652132137708,
     "user_tz": -120,
     "elapsed": 220,
     "user": {
      "displayName": "Danilo Rosenthal",
      "userId": "14867404785760024629"
     }
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sklearn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e914cea",
   "metadata": {
    "id": "4e914cea",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "a) Load Breast Cancer Wisconsin Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c51e501",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "0c51e501",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1652132139476,
     "user_tz": -120,
     "elapsed": 639,
     "user": {
      "displayName": "Danilo Rosenthal",
      "userId": "14867404785760024629"
     }
    },
    "outputId": "a35020e2-0408-451a-e8fc-f236e7e8140a",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "    1      2      3       4       5        6        7        8        9   \\\n0    M  17.99  10.38  122.80  1001.0  0.11840  0.27760  0.30010  0.14710   \n1    M  20.57  17.77  132.90  1326.0  0.08474  0.07864  0.08690  0.07017   \n2    M  19.69  21.25  130.00  1203.0  0.10960  0.15990  0.19740  0.12790   \n3    M  11.42  20.38   77.58   386.1  0.14250  0.28390  0.24140  0.10520   \n4    M  20.29  14.34  135.10  1297.0  0.10030  0.13280  0.19800  0.10430   \n..  ..    ...    ...     ...     ...      ...      ...      ...      ...   \n564  M  21.56  22.39  142.00  1479.0  0.11100  0.11590  0.24390  0.13890   \n565  M  20.13  28.25  131.20  1261.0  0.09780  0.10340  0.14400  0.09791   \n566  M  16.60  28.08  108.30   858.1  0.08455  0.10230  0.09251  0.05302   \n567  M  20.60  29.33  140.10  1265.0  0.11780  0.27700  0.35140  0.15200   \n568  B   7.76  24.54   47.92   181.0  0.05263  0.04362  0.00000  0.00000   \n\n         10  ...      22     23      24      25       26       27      28  \\\n0    0.2419  ...  25.380  17.33  184.60  2019.0  0.16220  0.66560  0.7119   \n1    0.1812  ...  24.990  23.41  158.80  1956.0  0.12380  0.18660  0.2416   \n2    0.2069  ...  23.570  25.53  152.50  1709.0  0.14440  0.42450  0.4504   \n3    0.2597  ...  14.910  26.50   98.87   567.7  0.20980  0.86630  0.6869   \n4    0.1809  ...  22.540  16.67  152.20  1575.0  0.13740  0.20500  0.4000   \n..      ...  ...     ...    ...     ...     ...      ...      ...     ...   \n564  0.1726  ...  25.450  26.40  166.10  2027.0  0.14100  0.21130  0.4107   \n565  0.1752  ...  23.690  38.25  155.00  1731.0  0.11660  0.19220  0.3215   \n566  0.1590  ...  18.980  34.12  126.70  1124.0  0.11390  0.30940  0.3403   \n567  0.2397  ...  25.740  39.42  184.60  1821.0  0.16500  0.86810  0.9387   \n568  0.1587  ...   9.456  30.37   59.16   268.6  0.08996  0.06444  0.0000   \n\n         29      30       31  \n0    0.2654  0.4601  0.11890  \n1    0.1860  0.2750  0.08902  \n2    0.2430  0.3613  0.08758  \n3    0.2575  0.6638  0.17300  \n4    0.1625  0.2364  0.07678  \n..      ...     ...      ...  \n564  0.2216  0.2060  0.07115  \n565  0.1628  0.2572  0.06637  \n566  0.1418  0.2218  0.07820  \n567  0.2650  0.4087  0.12400  \n568  0.0000  0.2871  0.07039  \n\n[569 rows x 31 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>10</th>\n      <th>...</th>\n      <th>22</th>\n      <th>23</th>\n      <th>24</th>\n      <th>25</th>\n      <th>26</th>\n      <th>27</th>\n      <th>28</th>\n      <th>29</th>\n      <th>30</th>\n      <th>31</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>M</td>\n      <td>17.99</td>\n      <td>10.38</td>\n      <td>122.80</td>\n      <td>1001.0</td>\n      <td>0.11840</td>\n      <td>0.27760</td>\n      <td>0.30010</td>\n      <td>0.14710</td>\n      <td>0.2419</td>\n      <td>...</td>\n      <td>25.380</td>\n      <td>17.33</td>\n      <td>184.60</td>\n      <td>2019.0</td>\n      <td>0.16220</td>\n      <td>0.66560</td>\n      <td>0.7119</td>\n      <td>0.2654</td>\n      <td>0.4601</td>\n      <td>0.11890</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>M</td>\n      <td>20.57</td>\n      <td>17.77</td>\n      <td>132.90</td>\n      <td>1326.0</td>\n      <td>0.08474</td>\n      <td>0.07864</td>\n      <td>0.08690</td>\n      <td>0.07017</td>\n      <td>0.1812</td>\n      <td>...</td>\n      <td>24.990</td>\n      <td>23.41</td>\n      <td>158.80</td>\n      <td>1956.0</td>\n      <td>0.12380</td>\n      <td>0.18660</td>\n      <td>0.2416</td>\n      <td>0.1860</td>\n      <td>0.2750</td>\n      <td>0.08902</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>M</td>\n      <td>19.69</td>\n      <td>21.25</td>\n      <td>130.00</td>\n      <td>1203.0</td>\n      <td>0.10960</td>\n      <td>0.15990</td>\n      <td>0.19740</td>\n      <td>0.12790</td>\n      <td>0.2069</td>\n      <td>...</td>\n      <td>23.570</td>\n      <td>25.53</td>\n      <td>152.50</td>\n      <td>1709.0</td>\n      <td>0.14440</td>\n      <td>0.42450</td>\n      <td>0.4504</td>\n      <td>0.2430</td>\n      <td>0.3613</td>\n      <td>0.08758</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>M</td>\n      <td>11.42</td>\n      <td>20.38</td>\n      <td>77.58</td>\n      <td>386.1</td>\n      <td>0.14250</td>\n      <td>0.28390</td>\n      <td>0.24140</td>\n      <td>0.10520</td>\n      <td>0.2597</td>\n      <td>...</td>\n      <td>14.910</td>\n      <td>26.50</td>\n      <td>98.87</td>\n      <td>567.7</td>\n      <td>0.20980</td>\n      <td>0.86630</td>\n      <td>0.6869</td>\n      <td>0.2575</td>\n      <td>0.6638</td>\n      <td>0.17300</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>M</td>\n      <td>20.29</td>\n      <td>14.34</td>\n      <td>135.10</td>\n      <td>1297.0</td>\n      <td>0.10030</td>\n      <td>0.13280</td>\n      <td>0.19800</td>\n      <td>0.10430</td>\n      <td>0.1809</td>\n      <td>...</td>\n      <td>22.540</td>\n      <td>16.67</td>\n      <td>152.20</td>\n      <td>1575.0</td>\n      <td>0.13740</td>\n      <td>0.20500</td>\n      <td>0.4000</td>\n      <td>0.1625</td>\n      <td>0.2364</td>\n      <td>0.07678</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>564</th>\n      <td>M</td>\n      <td>21.56</td>\n      <td>22.39</td>\n      <td>142.00</td>\n      <td>1479.0</td>\n      <td>0.11100</td>\n      <td>0.11590</td>\n      <td>0.24390</td>\n      <td>0.13890</td>\n      <td>0.1726</td>\n      <td>...</td>\n      <td>25.450</td>\n      <td>26.40</td>\n      <td>166.10</td>\n      <td>2027.0</td>\n      <td>0.14100</td>\n      <td>0.21130</td>\n      <td>0.4107</td>\n      <td>0.2216</td>\n      <td>0.2060</td>\n      <td>0.07115</td>\n    </tr>\n    <tr>\n      <th>565</th>\n      <td>M</td>\n      <td>20.13</td>\n      <td>28.25</td>\n      <td>131.20</td>\n      <td>1261.0</td>\n      <td>0.09780</td>\n      <td>0.10340</td>\n      <td>0.14400</td>\n      <td>0.09791</td>\n      <td>0.1752</td>\n      <td>...</td>\n      <td>23.690</td>\n      <td>38.25</td>\n      <td>155.00</td>\n      <td>1731.0</td>\n      <td>0.11660</td>\n      <td>0.19220</td>\n      <td>0.3215</td>\n      <td>0.1628</td>\n      <td>0.2572</td>\n      <td>0.06637</td>\n    </tr>\n    <tr>\n      <th>566</th>\n      <td>M</td>\n      <td>16.60</td>\n      <td>28.08</td>\n      <td>108.30</td>\n      <td>858.1</td>\n      <td>0.08455</td>\n      <td>0.10230</td>\n      <td>0.09251</td>\n      <td>0.05302</td>\n      <td>0.1590</td>\n      <td>...</td>\n      <td>18.980</td>\n      <td>34.12</td>\n      <td>126.70</td>\n      <td>1124.0</td>\n      <td>0.11390</td>\n      <td>0.30940</td>\n      <td>0.3403</td>\n      <td>0.1418</td>\n      <td>0.2218</td>\n      <td>0.07820</td>\n    </tr>\n    <tr>\n      <th>567</th>\n      <td>M</td>\n      <td>20.60</td>\n      <td>29.33</td>\n      <td>140.10</td>\n      <td>1265.0</td>\n      <td>0.11780</td>\n      <td>0.27700</td>\n      <td>0.35140</td>\n      <td>0.15200</td>\n      <td>0.2397</td>\n      <td>...</td>\n      <td>25.740</td>\n      <td>39.42</td>\n      <td>184.60</td>\n      <td>1821.0</td>\n      <td>0.16500</td>\n      <td>0.86810</td>\n      <td>0.9387</td>\n      <td>0.2650</td>\n      <td>0.4087</td>\n      <td>0.12400</td>\n    </tr>\n    <tr>\n      <th>568</th>\n      <td>B</td>\n      <td>7.76</td>\n      <td>24.54</td>\n      <td>47.92</td>\n      <td>181.0</td>\n      <td>0.05263</td>\n      <td>0.04362</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.1587</td>\n      <td>...</td>\n      <td>9.456</td>\n      <td>30.37</td>\n      <td>59.16</td>\n      <td>268.6</td>\n      <td>0.08996</td>\n      <td>0.06444</td>\n      <td>0.0000</td>\n      <td>0.0000</td>\n      <td>0.2871</td>\n      <td>0.07039</td>\n    </tr>\n  </tbody>\n</table>\n<p>569 rows × 31 columns</p>\n</div>"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wdbc.data', header=None)\n",
    "df = df.drop(columns=0)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "There are **30 features** and **1 binary label** (**M = malignant; B = benign**) per row. There are **569 data entries**."
   ],
   "metadata": {
    "id": "rfUkeZ21rBNS",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "id": "rfUkeZ21rBNS"
  },
  {
   "cell_type": "markdown",
   "source": [
    "b) Use **sklearn.preprocessing.LabelEncoder** to turn the binary labels into numberical values."
   ],
   "metadata": {
    "id": "avLi4mZurks1",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "id": "avLi4mZurks1"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "daf7d403",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "daf7d403",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1652132143045,
     "user_tz": -120,
     "elapsed": 418,
     "user": {
      "displayName": "Danilo Rosenthal",
      "userId": "14867404785760024629"
     }
    },
    "outputId": "2052e8ae-0d3a-4764-9b31-e28ddb3f9a20",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "     1      2      3       4       5        6        7        8        9   \\\n0     1  17.99  10.38  122.80  1001.0  0.11840  0.27760  0.30010  0.14710   \n1     1  20.57  17.77  132.90  1326.0  0.08474  0.07864  0.08690  0.07017   \n2     1  19.69  21.25  130.00  1203.0  0.10960  0.15990  0.19740  0.12790   \n3     1  11.42  20.38   77.58   386.1  0.14250  0.28390  0.24140  0.10520   \n4     1  20.29  14.34  135.10  1297.0  0.10030  0.13280  0.19800  0.10430   \n..   ..    ...    ...     ...     ...      ...      ...      ...      ...   \n564   1  21.56  22.39  142.00  1479.0  0.11100  0.11590  0.24390  0.13890   \n565   1  20.13  28.25  131.20  1261.0  0.09780  0.10340  0.14400  0.09791   \n566   1  16.60  28.08  108.30   858.1  0.08455  0.10230  0.09251  0.05302   \n567   1  20.60  29.33  140.10  1265.0  0.11780  0.27700  0.35140  0.15200   \n568   0   7.76  24.54   47.92   181.0  0.05263  0.04362  0.00000  0.00000   \n\n         10  ...      22     23      24      25       26       27      28  \\\n0    0.2419  ...  25.380  17.33  184.60  2019.0  0.16220  0.66560  0.7119   \n1    0.1812  ...  24.990  23.41  158.80  1956.0  0.12380  0.18660  0.2416   \n2    0.2069  ...  23.570  25.53  152.50  1709.0  0.14440  0.42450  0.4504   \n3    0.2597  ...  14.910  26.50   98.87   567.7  0.20980  0.86630  0.6869   \n4    0.1809  ...  22.540  16.67  152.20  1575.0  0.13740  0.20500  0.4000   \n..      ...  ...     ...    ...     ...     ...      ...      ...     ...   \n564  0.1726  ...  25.450  26.40  166.10  2027.0  0.14100  0.21130  0.4107   \n565  0.1752  ...  23.690  38.25  155.00  1731.0  0.11660  0.19220  0.3215   \n566  0.1590  ...  18.980  34.12  126.70  1124.0  0.11390  0.30940  0.3403   \n567  0.2397  ...  25.740  39.42  184.60  1821.0  0.16500  0.86810  0.9387   \n568  0.1587  ...   9.456  30.37   59.16   268.6  0.08996  0.06444  0.0000   \n\n         29      30       31  \n0    0.2654  0.4601  0.11890  \n1    0.1860  0.2750  0.08902  \n2    0.2430  0.3613  0.08758  \n3    0.2575  0.6638  0.17300  \n4    0.1625  0.2364  0.07678  \n..      ...     ...      ...  \n564  0.2216  0.2060  0.07115  \n565  0.1628  0.2572  0.06637  \n566  0.1418  0.2218  0.07820  \n567  0.2650  0.4087  0.12400  \n568  0.0000  0.2871  0.07039  \n\n[569 rows x 31 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>10</th>\n      <th>...</th>\n      <th>22</th>\n      <th>23</th>\n      <th>24</th>\n      <th>25</th>\n      <th>26</th>\n      <th>27</th>\n      <th>28</th>\n      <th>29</th>\n      <th>30</th>\n      <th>31</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>17.99</td>\n      <td>10.38</td>\n      <td>122.80</td>\n      <td>1001.0</td>\n      <td>0.11840</td>\n      <td>0.27760</td>\n      <td>0.30010</td>\n      <td>0.14710</td>\n      <td>0.2419</td>\n      <td>...</td>\n      <td>25.380</td>\n      <td>17.33</td>\n      <td>184.60</td>\n      <td>2019.0</td>\n      <td>0.16220</td>\n      <td>0.66560</td>\n      <td>0.7119</td>\n      <td>0.2654</td>\n      <td>0.4601</td>\n      <td>0.11890</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>20.57</td>\n      <td>17.77</td>\n      <td>132.90</td>\n      <td>1326.0</td>\n      <td>0.08474</td>\n      <td>0.07864</td>\n      <td>0.08690</td>\n      <td>0.07017</td>\n      <td>0.1812</td>\n      <td>...</td>\n      <td>24.990</td>\n      <td>23.41</td>\n      <td>158.80</td>\n      <td>1956.0</td>\n      <td>0.12380</td>\n      <td>0.18660</td>\n      <td>0.2416</td>\n      <td>0.1860</td>\n      <td>0.2750</td>\n      <td>0.08902</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>19.69</td>\n      <td>21.25</td>\n      <td>130.00</td>\n      <td>1203.0</td>\n      <td>0.10960</td>\n      <td>0.15990</td>\n      <td>0.19740</td>\n      <td>0.12790</td>\n      <td>0.2069</td>\n      <td>...</td>\n      <td>23.570</td>\n      <td>25.53</td>\n      <td>152.50</td>\n      <td>1709.0</td>\n      <td>0.14440</td>\n      <td>0.42450</td>\n      <td>0.4504</td>\n      <td>0.2430</td>\n      <td>0.3613</td>\n      <td>0.08758</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>11.42</td>\n      <td>20.38</td>\n      <td>77.58</td>\n      <td>386.1</td>\n      <td>0.14250</td>\n      <td>0.28390</td>\n      <td>0.24140</td>\n      <td>0.10520</td>\n      <td>0.2597</td>\n      <td>...</td>\n      <td>14.910</td>\n      <td>26.50</td>\n      <td>98.87</td>\n      <td>567.7</td>\n      <td>0.20980</td>\n      <td>0.86630</td>\n      <td>0.6869</td>\n      <td>0.2575</td>\n      <td>0.6638</td>\n      <td>0.17300</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1</td>\n      <td>20.29</td>\n      <td>14.34</td>\n      <td>135.10</td>\n      <td>1297.0</td>\n      <td>0.10030</td>\n      <td>0.13280</td>\n      <td>0.19800</td>\n      <td>0.10430</td>\n      <td>0.1809</td>\n      <td>...</td>\n      <td>22.540</td>\n      <td>16.67</td>\n      <td>152.20</td>\n      <td>1575.0</td>\n      <td>0.13740</td>\n      <td>0.20500</td>\n      <td>0.4000</td>\n      <td>0.1625</td>\n      <td>0.2364</td>\n      <td>0.07678</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>564</th>\n      <td>1</td>\n      <td>21.56</td>\n      <td>22.39</td>\n      <td>142.00</td>\n      <td>1479.0</td>\n      <td>0.11100</td>\n      <td>0.11590</td>\n      <td>0.24390</td>\n      <td>0.13890</td>\n      <td>0.1726</td>\n      <td>...</td>\n      <td>25.450</td>\n      <td>26.40</td>\n      <td>166.10</td>\n      <td>2027.0</td>\n      <td>0.14100</td>\n      <td>0.21130</td>\n      <td>0.4107</td>\n      <td>0.2216</td>\n      <td>0.2060</td>\n      <td>0.07115</td>\n    </tr>\n    <tr>\n      <th>565</th>\n      <td>1</td>\n      <td>20.13</td>\n      <td>28.25</td>\n      <td>131.20</td>\n      <td>1261.0</td>\n      <td>0.09780</td>\n      <td>0.10340</td>\n      <td>0.14400</td>\n      <td>0.09791</td>\n      <td>0.1752</td>\n      <td>...</td>\n      <td>23.690</td>\n      <td>38.25</td>\n      <td>155.00</td>\n      <td>1731.0</td>\n      <td>0.11660</td>\n      <td>0.19220</td>\n      <td>0.3215</td>\n      <td>0.1628</td>\n      <td>0.2572</td>\n      <td>0.06637</td>\n    </tr>\n    <tr>\n      <th>566</th>\n      <td>1</td>\n      <td>16.60</td>\n      <td>28.08</td>\n      <td>108.30</td>\n      <td>858.1</td>\n      <td>0.08455</td>\n      <td>0.10230</td>\n      <td>0.09251</td>\n      <td>0.05302</td>\n      <td>0.1590</td>\n      <td>...</td>\n      <td>18.980</td>\n      <td>34.12</td>\n      <td>126.70</td>\n      <td>1124.0</td>\n      <td>0.11390</td>\n      <td>0.30940</td>\n      <td>0.3403</td>\n      <td>0.1418</td>\n      <td>0.2218</td>\n      <td>0.07820</td>\n    </tr>\n    <tr>\n      <th>567</th>\n      <td>1</td>\n      <td>20.60</td>\n      <td>29.33</td>\n      <td>140.10</td>\n      <td>1265.0</td>\n      <td>0.11780</td>\n      <td>0.27700</td>\n      <td>0.35140</td>\n      <td>0.15200</td>\n      <td>0.2397</td>\n      <td>...</td>\n      <td>25.740</td>\n      <td>39.42</td>\n      <td>184.60</td>\n      <td>1821.0</td>\n      <td>0.16500</td>\n      <td>0.86810</td>\n      <td>0.9387</td>\n      <td>0.2650</td>\n      <td>0.4087</td>\n      <td>0.12400</td>\n    </tr>\n    <tr>\n      <th>568</th>\n      <td>0</td>\n      <td>7.76</td>\n      <td>24.54</td>\n      <td>47.92</td>\n      <td>181.0</td>\n      <td>0.05263</td>\n      <td>0.04362</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.1587</td>\n      <td>...</td>\n      <td>9.456</td>\n      <td>30.37</td>\n      <td>59.16</td>\n      <td>268.6</td>\n      <td>0.08996</td>\n      <td>0.06444</td>\n      <td>0.0000</td>\n      <td>0.0000</td>\n      <td>0.2871</td>\n      <td>0.07039</td>\n    </tr>\n  </tbody>\n</table>\n<p>569 rows × 31 columns</p>\n</div>"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Use LabelEncoder to transform the Lable into numeric features\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "lb_enc = LabelEncoder()\n",
    "df[1] = lb_enc.fit_transform(df[1])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "**M=1** and **B=0**"
   ],
   "metadata": {
    "id": "j7Ac27dHvvs9",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "id": "j7Ac27dHvvs9"
  },
  {
   "cell_type": "markdown",
   "source": [
    "c) Use **sklearn.cross_validation.train_test_split** to split the data set into trainings (80%) and test (20%) data. Set **random_state=1**."
   ],
   "metadata": {
    "id": "cFci489espCr",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "id": "cFci489espCr"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d7d50994",
   "metadata": {
    "id": "d7d50994",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1652132145401,
     "user_tz": -120,
     "elapsed": 223,
     "user": {
      "displayName": "Danilo Rosenthal",
      "userId": "14867404785760024629"
     }
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Split the data into 80% train and 20% test data \n",
    "from sklearn.model_selection import train_test_split\n",
    "data = df.drop(columns=1)\n",
    "target= df[1]\n",
    "x_train, x_test, y_train, y_test = train_test_split(data, target, test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "d) Use **sklearn.preprocessing.StandardScaler** for preprocessing (feature scaling) and PCA for dimensionality reduction with **sklearn.decomposition.PCA** and n_components=2. Use **sklearn.linear_model.LogisticRegression** as classifier with **random_state=1**. Put everything into a pipeline (**sklearn.pipeline.Pipeline)**."
   ],
   "metadata": {
    "id": "WTuWnu8WtCoo",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "id": "WTuWnu8WtCoo"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4c83aad6",
   "metadata": {
    "id": "4c83aad6",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1652132148377,
     "user_tz": -120,
     "elapsed": 435,
     "user": {
      "displayName": "Danilo Rosenthal",
      "userId": "14867404785760024629"
     }
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0bab7b2d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0bab7b2d",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1652132149811,
     "user_tz": -120,
     "elapsed": 310,
     "user": {
      "displayName": "Danilo Rosenthal",
      "userId": "14867404785760024629"
     }
    },
    "outputId": "2d2e7f8e-642c-48cc-8308-65fa3c24c05c",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "Pipeline(steps=[('scaler', StandardScaler()), ('pca', PCA(n_components=2)),\n                ('clf', LogisticRegression(random_state=1))])"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Setup a pipeline, which standardize the data for a PCA and applies a LogisticRegression on the data\n",
    "pipe = Pipeline(steps=[\n",
    "    ('scaler', StandardScaler()),('pca',PCA(n_components=2)),('clf',LogisticRegression(random_state=1))])\n",
    "pipe.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "e) Test with **pipeline.score** your model's accuracy. You should receive an accuracy of **0.947**."
   ],
   "metadata": {
    "id": "B7XWvvsLty8E",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "id": "B7XWvvsLty8E"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "89a04399",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "89a04399",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1652132153072,
     "user_tz": -120,
     "elapsed": 304,
     "user": {
      "displayName": "Danilo Rosenthal",
      "userId": "14867404785760024629"
     }
    },
    "outputId": "ddc66e18-6ca4-4718-d891-5a9041216012",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "0.9473684210526315"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.score(x_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Accuracy of **0.947** (94%). "
   ],
   "metadata": {
    "id": "ZIaKqjm2uD9L",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "id": "ZIaKqjm2uD9L"
  },
  {
   "cell_type": "markdown",
   "source": [
    "f) Use **Recursive Feature Elimination** (**RFE**; **sklearn.feature_selection.RFECV**) for feature selection instead of PCA. Which and how many features are interesting for classification? Which (max.) accuracy on test data be reached with this step (instead of PCA)?"
   ],
   "metadata": {
    "id": "-qBNStLPuPTg",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "id": "-qBNStLPuPTg"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2b0bcdc0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2b0bcdc0",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1652132158088,
     "user_tz": -120,
     "elapsed": 1862,
     "user": {
      "displayName": "Danilo Rosenthal",
      "userId": "14867404785760024629"
     }
    },
    "outputId": "70e61cf1-2e9f-4427-f259-32c894499ab1",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "array([ True,  True,  True,  True, False,  True,  True,  True, False,\n       False,  True, False,  True,  True, False,  True, False,  True,\n       False,  True,  True,  True,  True,  True,  True, False,  True,\n        True,  True,  True])"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Use the pipeline with a RFECV instead of a PCA and show wich features are selected.\n",
    "from sklearn.feature_selection import RFECV\n",
    "pipe2 = Pipeline(steps=[\n",
    "    ('scaler', StandardScaler()),('rfecv',RFECV(estimator=LogisticRegression(random_state=1)))])\n",
    "pipe2.fit(x_train,y_train)\n",
    "pipe2['rfecv'].support_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7c4d36cb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7c4d36cb",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1652132160234,
     "user_tz": -120,
     "elapsed": 400,
     "user": {
      "displayName": "Danilo Rosenthal",
      "userId": "14867404785760024629"
     }
    },
    "outputId": "ff366492-9538-4320-a0bd-0aacbb30d6b5",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features: 22\n"
     ]
    }
   ],
   "source": [
    "print('Number of features:',sum(pipe2['rfecv'].support_==True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d6b97cf4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d6b97cf4",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1652132162401,
     "user_tz": -120,
     "elapsed": 232,
     "user": {
      "displayName": "Danilo Rosenthal",
      "userId": "14867404785760024629"
     }
    },
    "outputId": "d4ad07d6-94e9-4f3c-f286-995c2cb4d1f8",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indices of the selected features: [2.0, 3.0, 4.0, 5.0, 7.0, 8.0, 9.0, 12.0, 14.0, 15.0, 17.0, 19.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 28.0, 29.0, 30.0, 31.0]\n"
     ]
    }
   ],
   "source": [
    "imp_feature=x_test.columns.where(pipe2['rfecv'].support_==True).dropna().tolist()\n",
    "print('Indices of the selected features:',imp_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "95d89ac5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "95d89ac5",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1652132163856,
     "user_tz": -120,
     "elapsed": 188,
     "user": {
      "displayName": "Danilo Rosenthal",
      "userId": "14867404785760024629"
     }
    },
    "outputId": "523a338b-fa93-4782-bd11-53409a876a75",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "0.9824561403508771"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe2.score(x_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "REF selects **22** features and we can reach an accuracy of **0.982** (98%)."
   ],
   "metadata": {
    "id": "TyFXHtuxvD15",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "id": "TyFXHtuxvD15"
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "colab": {
   "name": "Exercise_7.ipynb",
   "provenance": [],
   "collapsed_sections": []
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}