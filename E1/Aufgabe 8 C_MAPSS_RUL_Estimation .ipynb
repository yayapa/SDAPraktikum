{"cells":[{"cell_type":"markdown","metadata":{"id":"e96OGEl07oXG"},"source":["# Mount Google Drive\n","In this example, the data is saved in the folder of personal google drive.\n","\n","First you have to upload the data to your google drive, then connect the drive.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"skgwbdhJ2aEl"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","metadata":{"id":"CG_U0Y9w72ze"},"source":["# Install Python Packages\n","Although most of the commonly used Python libraries are pre-installed, new libraries can be installed using the below packages:\n","\n","!pip install [package name]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vGHEFxAJ9oqF"},"outputs":[],"source":["!pip install tsfresh\n","# tsfresh is a python package, which automatically calculates a large number of time series characteristics (features)"]},{"cell_type":"markdown","metadata":{"id":"A67lPTNHJkGs"},"source":["# Import Packages"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"boLQLTsdJjY6"},"outputs":[],"source":["import pandas as pd\n","import seaborn as sns\n","import numpy as np\n","sns.set()\n","import matplotlib.pyplot as plt\n","plt.rcParams['ytick.labelsize'] = \"x-large\"\n","plt.rcParams['xtick.labelsize'] = \"x-large\"\n","plt.rcParams['axes.labelsize'] = \"x-large\"\n","plt.rcParams['figure.titlesize'] = \"x-large\"\n","\n","\n","from sklearn.preprocessing import normalize\n","import lightgbm as lgb\n","\n","from sklearn.ensemble import RandomForestRegressor\n","from sklearn.linear_model import LassoCV\n","\n","from sklearn.metrics import mean_squared_error\n","\n","import keras\n","\n","from tensorflow.keras.models import Sequential, load_model\n","from tensorflow.keras.layers import Dense\n","from tensorflow.keras.layers import Flatten\n","from tensorflow.keras.layers import TimeDistributed\n","\n","# Setting seed for reproducability\n","np.random.seed(1234)  \n","PYTHONHASHSEED = 0\n","from sklearn import preprocessing\n","from sklearn.metrics import confusion_matrix, recall_score, precision_score\n","from keras.models import Sequential\n","from keras.layers import Dense, Dropout, LSTM, Activation\n","%matplotlib inline"]},{"cell_type":"markdown","metadata":{"id":"wb00bcxu9tih"},"source":["# Data Description\n","C-MAPSS data set which contains turbofan engine degradation data is a widely used prognostic benchmark data for predicting the Remaining useful life (RUL). This data set is simulated by the tool Commercial Modular Aero Propulsion System Simulation (C-MAPSS) developed by NASA. Run to failure simulations were performed for engines with varying degrees of initial wear but in a healthy state. During each cycle in the simulation, one sample of all 21 sensors such as physical core speed, temperature at fan inlet and pressure at fan inlet etc will be recorded once. As the simulation progresses, the performance of the turbofan engine degrades until it loses functionality. \n","\n","C-MAPSS data consists of four sub-data sets with different operational conditions and fault patterns. \n","\n","|         Dataset        | FD001 | FD002 | FD003 | FD004 |\n","|:----------------------:|:-----:|:-----:|:-----:|:-----:|\n","|      Training set      |  100  |  260  |  100  |  249  |\n","|        Test set        |  100  |  259  |  100  |  248  |\n","| Operational conditions |   1   |   6   |   1   |   6   |\n","| Fault conditions       | 1     | 1     | 2     | 2     |\n","\n","\n","As shown Table above, each sub-data set has been split into a training set and a test set. The training sets contain sensor records for all cycles in the run to failure simulation. Unlike the training sets, the test sets only contain partial temporal sensor records which stopped at a time prior to the failure. The task is to predict the RUL of each engine in the test sets by using the training sets with the given sensor records. The corresponding RUL to test sets has been provided. With this, the performance of the model can be verified. \n","\n","The data provieded as text file with 26 columns of numbers, separated by spaces. Each row is a snapshot of data taken during a single operational cycle, each column is a different variable. The columns correspond to:\n","\n","unit number\n","\n","time, in cycles\n","\n","operational setting 1\n","\n","operational setting 2\n","\n","operational setting 3\n","\n","sensor measurement 1\n","\n","sensor measurement 2 \n","\n","sensor measurement 3 \n","\n","...\n","\n","sensor measurement 26"]},{"cell_type":"markdown","metadata":{"id":"VLw-w1TDE9Zo"},"source":["# Data Exploration and Preparation\n","take FD001 as example"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QAmDXiZU2mkK"},"outputs":[],"source":["# Load the Data\n","Path_to_data = \"drive/My Drive/PSDA2020/\"\n","\n","column_name = ['engine_id', 'cycle', 'setting1', 'setting2', 'setting3', 's1', 's2', 's3',\n","               's4', 's5', 's6', 's7', 's8', 's9', 's10', 's11', 's12', 's13', 's14',\n","               's15', 's16', 's17', 's18', 's19', 's20', 's21']\n","# training data set\n","train_FD001 = pd.read_table(Path_to_data+\"train_FD001.txt\", header=None, delim_whitespace=True)\n","train_FD001.columns = column_name\n","\n","# test data set\n","test_FD001 = pd.read_table(Path_to_data+\"test_FD001.txt\", header=None, delim_whitespace=True)\n","test_FD001.columns = column_name\n","\n","# RUL for test data set\n","RUL_FD001 = pd.read_table(Path_to_data+\"RUL_FD001.txt\", header=None, delim_whitespace=True)\n","\n","train_FD001.head()"]},{"cell_type":"markdown","metadata":{"id":"g2R6-hIDIJZu"},"source":["In this sub dataset we have **100** engines (engine_id) which are monitored over time (cycle). Each engine had operational_settings and sensor_measurements recorded for each cycle. The RUL is the amount of cycles an engine has left before it needs maintenance. What makes this data set special is that the engines run all the way until failure, giving us precise RUL information for every engine at every point in time."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7z2IzRdSIdDk"},"outputs":[],"source":["def add_RUL(col):\n","    # Reverse the cycle evolution, where remaining time of a machine is 0 at the failure.\n","    # It is assumed here that the state of the machine is linearly deteriorating\n","    return col[::-1]-1\n","# Calculate RUL for each time point of each engine  \n","train_FD001['rul'] = train_FD001[['engine_id', 'cycle']].groupby('engine_id').transform(add_RUL)"]},{"cell_type":"markdown","metadata":{"id":"hQMbZSIAE1aU"},"source":["**Is there any other way to define target lable (RUL) ?** "]},{"cell_type":"code","source":["#Plotting an engine's classic linear degradation function\n","engine_id = 1\n","df_of_id = train_FD001[train_FD001['engine_id']==engine_id]\n","plt.plot(df_of_id.rul, label=\"RUL\")\n","#plt.plot(df_of_id.Cycle, label=\"Cycle\")\n","plt.legend()\n","plt.title(\"Classic linear degrading RUL of engine {}\".format(engine_id))\n","plt.show()"],"metadata":{"id":"NF9KQLunesvy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["A second way to define the target label/Remaining useful lifetime (RUL) is called piece-wise linear degradation function. The method above is a classic linear degradation function.\n","The piece-wise degradation function assumes that the degradation of the RUL starts at a specific point in lifetime. Before that point the function is constant, after that point it starts degrading linearly. This point of degradation can be seen in graphs below when the curves of specific sensor's values start to increase or decrease steadily over time."],"metadata":{"id":"VuxWr2iee_pa"}},{"cell_type":"markdown","source":["Piece-wise linear degradation function. Warning before running cell: overrides linear degradation function above!"],"metadata":{"id":"n4mEBfGR_qF7"}},{"cell_type":"code","source":["# piece-wise linear degradation function\n","id='engine_id'\n","MAXLIFE = 120 # or 125 , 130\n","rul = [] \n","for _id in set(train_FD001[id]):\n","    train_FD001_id =  train_FD001[train_FD001[id] == _id]\n","    cycle_list = train_FD001_id['cycle'].tolist()\n","    max_cycle = max(cycle_list)\n","\n","    breaking_point = max_cycle - MAXLIFE\n","    kink_RUL = []\n","    for i in range(0, len(cycle_list)):\n","        if i < breaking_point:\n","            kink_RUL.append(MAXLIFE)\n","        else:\n","            tmp = max_cycle-i-1\n","            kink_RUL.append(tmp)\n","    rul.extend(kink_RUL)\n","\n","train_FD001['rul'] = rul"],"metadata":{"id":"OEd1Py1egF9G"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Plotting piece-wise linear degradation function."],"metadata":{"id":"aVIk6qBR_wzt"}},{"cell_type":"code","source":["#Plotting an engine's piece-wise linear degradation function\n","engine_id = 1\n","df_of_id = train_FD001[train_FD001['engine_id']==engine_id]\n","plt.plot(df_of_id.rul, label=\"RUL\")\n","#plt.plot(df_of_id.Cycle, label=\"Cycle\")\n","plt.legend()\n","plt.title(\"piece-wise linear degrading RUL of engine {}\".format(engine_id))\n","plt.show()"],"metadata":{"id":"ke2NImvDdus4"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YfBy89S6P4kJ"},"outputs":[],"source":["# Visualize the RUL curve of some engines (1,2,3,4,5,6)\n","g = sns.PairGrid(data=train_FD001.reset_index().query('engine_id < 7') ,\n","                 x_vars=[\"index\"],\n","                 y_vars=['rul'],\n","                 hue=\"engine_id\", height=3, aspect=2.5)\n","\n","g = g.map(plt.plot, alpha=1)\n","g = g.add_legend()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-YfEPxGjLm5K"},"outputs":[],"source":["# Visualize some sensor curves of some engines \n","g = sns.PairGrid(data=train_FD001.query('engine_id < 5') ,\n","                 x_vars=[\"rul\"],\n","                 y_vars=['s1','s2'],\n","                 hue=\"engine_id\", height=3, aspect=2.5)\n","\n","g = g.map(plt.plot, alpha=1)\n","g = g.add_legend()\n","\n","# As shown in the figure, some sensors are not related to RUL. \n","# The values of some sensors change with the state of the machine. \n","# Visualization can help filter features"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6kbeakQcTv5n"},"outputs":[],"source":["# Distribution of maximum life cycle\n","train_FD001[['engine_id', 'rul']].groupby('engine_id').apply(np.max)[\"rul\"].hist(bins=20)\n","plt.xlabel(\"max life cycle\")\n","plt.ylabel(\"Count\")\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"5urHHKAeFKY7"},"source":["**Can you do more visualization？ Please give a simple summary or explanation for each visualization**"]},{"cell_type":"code","source":["# The following graphs show the change in value of every sensor over time (here for engine number one). X-axis: cycle number, Y-axis: sensor values\n","engine_id = 1\n","plt.figure(figsize=(15,2.5*21))\n","Dataframe_id = train_FD001[train_FD001[\"engine_id\"]==engine_id]\n","for i in range(21):\n","    a = plt.subplot(26,1,i+1)\n","    a.plot(Dataframe_id.index.values,Dataframe_id.iloc[:,i+5].values)\n","    a.title.set_text(\"Sensor \" + str(i+1) + \", column \"+ str(i+6))\n","    plt.tight_layout()\n","plt.show()\n","\n"],"metadata":{"id":"rgKdOFlKxEz9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["It can be seen that most sensors either tend to go up or tend to go down over time. That could be caused by the change of the engine's state. Other sensors (1, 5, 6, 10, 16, 18, 19) have constant values over time. "],"metadata":{"id":"7lOVxubK1C9q"}},{"cell_type":"code","source":["# Boxplots of sensor data.\n","plt.figure(figsize = (16, 21))\n","\n","for i in range(21):\n","    temp_data = train_FD001.iloc[:,i+5]\n","    plt.subplot(7,3,i+1)\n","    plt.boxplot(temp_data)\n","    plt.title(\"Sensor \" + str(i+1) + \", column \"+ str(i+6))\n","plt.show()"],"metadata":{"id":"oz0zUOqfptNY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["It can be seen, that sensors 1, 5, 6, 10, 16, 18 and 19 have constant values over time. That means that these sensors seem to have no further impact on the lifespan of an engine. It is indicted that only sensors 2, 3, 4, 7, 8, 9, 11, 12 ,13 ,14 ,15 ,17, 20, 21 need to be considered in further calculations/visualizations."],"metadata":{"id":"j1jt_riUuxaB"}},{"cell_type":"code","source":["sensor_names = column_name[5:]\n","train_FD001[sensor_names].describe().transpose()"],"metadata":{"id":"adKHu1Lg7fUM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The standard deviation of sensors 18 and 19 clearly show that they are obsolete for RUL since they hold no useful information. Sensors 1, 5, 10 and 16 only have a very small standard deviation."],"metadata":{"id":"nE4Z3UbO9a8q"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"HwWNM2GQWE-Q"},"outputs":[],"source":["# prepare the data and normalization\n","train_y = train_FD001['rul']\n","features = train_FD001.columns.drop(['engine_id', 'cycle', 'rul'])\n","train_x = train_FD001[features]\n","test_x = test_FD001[features]\n","\n","\n","# z score normalization\n","mean = train_x.mean()\n","std = train_x.std()\n","std.replace(0, 1, inplace=True)\n","\n","train_x = (train_x - mean) / std\n","test_x = (test_x - mean) / std\n","\n","from sklearn.utils import shuffle\n","x, y = shuffle(train_x, train_y)\n","train_FD001.head()\n","\n"]},{"cell_type":"markdown","metadata":{"id":"lNk2ycyuFrHq"},"source":["**Here only the values at each time point (cycle) are used to predict the RUL. Temporal relationship is ignored. How to use Temporal relationship?**\n","\n","tip : Sliding Window\n"]},{"cell_type":"markdown","source":["Sliding Window can be used to capture temporal relationships in a dataset. A window is pushed over the data whih are then combined. Window size and stride are important parameters. For an example, see LSTM model below."],"metadata":{"id":"CtSG1Fa9AJVu"}},{"cell_type":"markdown","metadata":{"id":"oZPWL4IWVJFV"},"source":["# Modeling"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"alPz0At-SyVz"},"outputs":[],"source":["# Random Forest with default Hyper parameters\n","rf_model = RandomForestRegressor()\n","rf_model.fit(x,y)\n","rf_prediction = rf_model.predict(train_x)\n","plt.plot(rf_prediction[:500])\n","plt.plot(train_FD001[\"rul\"][:500])\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dDzgpXQcLGsC"},"outputs":[],"source":["# Lasso model with default Hyper parameters\n","ls_model = LassoCV()\n","ls_model.fit(x,y)\n","ls_prediction = ls_model.predict(train_x)\n","plt.plot(ls_prediction[:500])\n","plt.plot(train_FD001[\"rul\"][:500])\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"I12xg2WDK0pD"},"source":["**How to tune hyperparameters and select models?** \n","\n","**Neural network model?**"]},{"cell_type":"markdown","source":["In Predictive Maintenance regression models are used to predict Remaining Useful Lifetime (RUL).\n","Model selection: What kind of output should the model give? Availability of sufficient historical data? Here: Classification problem of time-series data! Therefore (Deep) CNNs and LSTMs should be chosen. As results indicate/prove.\n","Hyperparameter tuning: Specify which (hyper-)parameters are to tune. First, start with more or less random values. Then opitmizer like BOHB can be used. BOHB consists of the Bayesian optimization (BO) and the Hyperband (HB)."],"metadata":{"id":"CpcsQqtxxu5N"}},{"cell_type":"code","source":["# LGBMRegressor with default Hyper parameters\n","reg_model = lgb.LGBMRegressor(random_state=12)\n","reg_model.fit(x, y)\n","reg_prediction = reg_model.predict(train_x)\n","#plt.figure(figsize=(12,5))\n","plt.plot((reg_prediction[:2000]), label=\"Prediction\")\n","plt.plot(train_FD001[\"rul\"][:2000])\n","plt.legend()\n","plt.show()"],"metadata":{"id":"Vvzl-9QKiQct"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0iQ_3EzOVeAe"},"source":["# Evaluation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ie-jLvidHZ9O"},"outputs":[],"source":["\n","# Since only the value at one time point is used, it can be seen that a lot of data in the test set is not used\n","\n","test_x['engine_id'] = test_FD001['engine_id']\n","test_input = []\n","for id in test_x['engine_id'].unique():\n","  \n","  test_input.append(test_x[test_x['engine_id']==id].iloc[-1,:-1].values)\n","\n","test_input = np.array(test_input)\n","test_input"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fJ1HI43iHZ65"},"outputs":[],"source":["# Random forest\n","\n","rf_test_prediction = rf_model.predict(test_input)\n","\n","rf_rmse = np.sqrt(mean_squared_error(rf_test_prediction, RUL_FD001.values.reshape(-1)))\n","\n","print(\"The RMSE of random forest on test dataset FD001 is \",rf_rmse)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0YW_6FmUHZ4T"},"outputs":[],"source":["# Lasso model\n","\n","ls_test_prediction = ls_model.predict(test_input)\n","\n","ls_rmse = np.sqrt(mean_squared_error(ls_test_prediction, RUL_FD001.values.reshape(-1)))\n","\n","print(\"The RMSE of Lasso model on test dataset FD001 is \",ls_rmse)"]},{"cell_type":"code","source":["# LGBMRegressor model\n","\n","reg_test_prediction = reg_model.predict(test_input)\n","\n","reg_rmse = np.sqrt(mean_squared_error(reg_test_prediction, RUL_FD001.values.reshape(-1)))\n","\n","print(\"The RMSE of LGBMRegressor model on test dataset FD001 is \",reg_rmse)"],"metadata":{"id":"B714fVwWiwY9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"myaoeDxY09PE"},"source":["**What is your best result? If the used model is interpretable, what other conclusions can be summarized**"]},{"cell_type":"markdown","source":["Here we use LSTM with sliding window and reach a better solution than Random Forest model, Lasso model and LGBMRegression model."],"metadata":{"id":"LHFOjPei-MI4"}},{"cell_type":"code","source":["# read training data \n","train_df = pd.read_csv(\"drive/My Drive/PSDA2020/train_FD001.txt\", sep=\" \", header=None)\n","train_df.drop(train_df.columns[[26, 27]], axis=1, inplace=True)\n","train_df.columns = ['id', 'cycle', 'setting1', 'setting2', 'setting3', 's1', 's2', 's3',\n","                     's4', 's5', 's6', 's7', 's8', 's9', 's10', 's11', 's12', 's13', 's14',\n","                     's15', 's16', 's17', 's18', 's19', 's20', 's21']"],"metadata":{"id":"qbNuvKku4kH5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# read test data\n","test_df = pd.read_csv(\"drive/My Drive/PSDA2020/test_FD001.txt\", sep=\" \", header=None)\n","test_df.drop(test_df.columns[[26, 27]], axis=1, inplace=True)\n","test_df.columns = ['id', 'cycle', 'setting1', 'setting2', 'setting3', 's1', 's2', 's3',\n","                     's4', 's5', 's6', 's7', 's8', 's9', 's10', 's11', 's12', 's13', 's14',\n","                     's15', 's16', 's17', 's18', 's19', 's20', 's21']"],"metadata":{"id":"kZFiDHLL4tl_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# read ground truth data\n","truth_df = pd.read_csv(\"drive/My Drive/PSDA2020/RUL_FD001.txt\", sep=\" \", header=None)\n","truth_df.drop(truth_df.columns[[1]], axis=1, inplace=True)"],"metadata":{"id":"M_PM0QES4wFb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_df = train_df.sort_values(['id','cycle'])\n","train_df.head()"],"metadata":{"id":"icYwZYzH40Hy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Data Labeling - generate column RUL\n","rul = pd.DataFrame(train_df.groupby('id')['cycle'].max()).reset_index()\n","rul.columns = ['id', 'max']\n","train_df = train_df.merge(rul, on=['id'], how='left')\n","train_df['RUL'] = train_df['max'] - train_df['cycle']\n","train_df.drop('max', axis=1, inplace=True)\n","train_df.head()"],"metadata":{"id":"86sXCBVm44pX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# generate label columns for training data\n","w1 = 30\n","w0 = 15\n","train_df['label1'] = np.where(train_df['RUL'] <= w1, 1, 0 )\n","train_df['label2'] = train_df['label1']\n","train_df.loc[train_df['RUL'] <= w0, 'label2'] = 2\n","train_df.head()"],"metadata":{"id":"ktIMeIIG48Ck"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# MinMax normalization\n","train_df['cycle_norm'] = train_df['cycle']\n","cols_normalize = train_df.columns.difference(['id','cycle','RUL','label1','label2'])\n","min_max_scaler = preprocessing.MinMaxScaler()\n","norm_train_df = pd.DataFrame(min_max_scaler.fit_transform(train_df[cols_normalize]), \n","                             columns=cols_normalize, \n","                             index=train_df.index)\n","join_df = train_df[train_df.columns.difference(cols_normalize)].join(norm_train_df)\n","train_df = join_df.reindex(columns = train_df.columns)\n","train_df.head()"],"metadata":{"id":"Mp3-_em-4-3I"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_df['cycle_norm'] = test_df['cycle']\n","norm_test_df = pd.DataFrame(min_max_scaler.transform(test_df[cols_normalize]), \n","                            columns=cols_normalize, \n","                            index=test_df.index)\n","test_join_df = test_df[test_df.columns.difference(cols_normalize)].join(norm_test_df)\n","test_df = test_join_df.reindex(columns = test_df.columns)\n","test_df = test_df.reset_index(drop=True)\n","test_df.head()"],"metadata":{"id":"xw_GNTo65Bib"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# generate column max for test data\n","rul = pd.DataFrame(test_df.groupby('id')['cycle'].max()).reset_index()\n","rul.columns = ['id', 'max']\n","truth_df.columns = ['more']\n","truth_df['id'] = truth_df.index + 1\n","truth_df['max'] = rul['max'] + truth_df['more']\n","truth_df.drop('more', axis=1, inplace=True)"],"metadata":{"id":"52K4NcOv5EU2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# generate RUL for test data\n","test_df = test_df.merge(truth_df, on=['id'], how='left')\n","test_df['RUL'] = test_df['max'] - test_df['cycle']\n","test_df.drop('max', axis=1, inplace=True)\n","test_df.head()"],"metadata":{"id":"s6fhVJkf5Gsm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# generate label columns label1 and label2 for test data\n","test_df['label1'] = np.where(test_df['RUL'] <= w1, 1, 0 )\n","test_df['label2'] = test_df['label1']\n","test_df.loc[test_df['RUL'] <= w0, 'label2'] = 2\n","test_df.head()"],"metadata":{"id":"a00iBac15JLc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# pick a window size of 50 cycles\n","sequence_length = 50"],"metadata":{"id":"_gkk5swt5Mch"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# preparing data for visualizations \n","engine_id3 = test_df[test_df['id'] == 3]\n","engine_id3_50cycleWindow = engine_id3[engine_id3['RUL'] <= engine_id3['RUL'].min() + 50]\n","cols1 = ['s1', 's2', 's3', 's4', 's5', 's6', 's7', 's8', 's9', 's10']\n","engine_id3_50cycleWindow1 = engine_id3_50cycleWindow[cols1]\n","cols2 = ['s11', 's12', 's13', 's14', 's15', 's16', 's17', 's18', 's19', 's20', 's21']\n","engine_id3_50cycleWindow2 = engine_id3_50cycleWindow[cols2]"],"metadata":{"id":"EaFjEcWe5aHb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# function to reshape features into (samples, time steps, features) \n","def gen_sequence(id_df, seq_length, seq_cols):\n","    \"\"\" Only sequences that meet the window-length are considered, no padding is used. This means for testing\n","    we need to drop those which are below the window-length. An alternative would be to pad sequences so that\n","    we can use shorter ones \"\"\"\n","    data_array = id_df[seq_cols].values\n","    num_elements = data_array.shape[0]\n","    for start, stop in zip(range(0, num_elements-seq_length), range(seq_length, num_elements)):\n","        yield data_array[start:stop, :]"],"metadata":{"id":"zPKUzTNz5iO3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# pick the feature columns \n","sensor_cols = ['s' + str(i) for i in range(1,22)]\n","sequence_cols = ['setting1', 'setting2', 'setting3', 'cycle_norm']\n","sequence_cols.extend(sensor_cols)"],"metadata":{"id":"yT6DAGEp5pMS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# generator for the sequences\n","seq_gen = (list(gen_sequence(train_df[train_df['id']==id], sequence_length, sequence_cols)) \n","           for id in train_df['id'].unique())"],"metadata":{"id":"NuMKPgzK5rPk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# generate sequences and convert to numpy array\n","seq_array = np.concatenate(list(seq_gen)).astype(np.float32)\n","seq_array.shape"],"metadata":{"id":"uRsJOEoh5tcT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# function to generate labels\n","def gen_labels(id_df, seq_length, label):\n","    data_array = id_df[label].values\n","    num_elements = data_array.shape[0]\n","    return data_array[seq_length:num_elements, :]"],"metadata":{"id":"QfNNgCyC5vgB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# generate labels\n","label_gen = [gen_labels(train_df[train_df['id']==id], sequence_length, ['label1']) \n","             for id in train_df['id'].unique()]\n","label_array = np.concatenate(label_gen).astype(np.float32)\n","label_array.shape"],"metadata":{"id":"Gdh1t7Ut5x_B"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# build the network\n","nb_features = seq_array.shape[2]\n","nb_out = label_array.shape[1]\n","\n","model = Sequential()\n","\n","model.add(LSTM(\n","         input_shape=(sequence_length, nb_features),\n","         units=100,\n","         return_sequences=True))\n","model.add(Dropout(0.2))\n","\n","model.add(LSTM(\n","          units=50,\n","          return_sequences=False))\n","model.add(Dropout(0.2))\n","\n","model.add(Dense(units=nb_out, activation='sigmoid'))\n","model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n","\n","print(model.summary())"],"metadata":{"id":"odFUagOE5z-B"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%%time\n","# fit the network\n","model.fit(seq_array, label_array, epochs=10, batch_size=200, validation_split=0.05, verbose=1,\n","          callbacks = [keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=0, verbose=0, mode='auto')])"],"metadata":{"id":"nYDyZYD255gv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# training metrics\n","scores = model.evaluate(seq_array, label_array, verbose=1, batch_size=200)\n","print('Accurracy: {}'.format(scores[1]))"],"metadata":{"id":"KM48GkZ458UM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# make predictions and compute confusion matrix\n","y_pred = (model.predict(seq_array) > 0.5).astype(\"int32\")\n","y_true = label_array\n","print('Confusion matrix\\n- x-axis is true labels.\\n- y-axis is predicted labels')\n","cm = confusion_matrix(y_true, y_pred)\n","cm"],"metadata":{"id":"ieGRGDXG_TDu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# compute precision and recall\n","precision = precision_score(y_true, y_pred)\n","recall = recall_score(y_true, y_pred)\n","print( 'precision = ', precision, '\\n', 'recall = ', recall)"],"metadata":{"id":"tkvY3gCG1-5Y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["seq_array_test_last = [test_df[test_df['id']==id][sequence_cols].values[-sequence_length:] \n","                       for id in test_df['id'].unique() if len(test_df[test_df['id']==id]) >= sequence_length]\n","\n","seq_array_test_last = np.asarray(seq_array_test_last).astype(np.float32)\n","seq_array_test_last.shape"],"metadata":{"id":"daqvTmj6B7zA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["y_mask = [len(test_df[test_df['id']==id]) >= sequence_length for id in test_df['id'].unique()]"],"metadata":{"id":"l5jPJJ4bB982"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["label_array_test_last = test_df.groupby('id')['label1'].nth(-1)[y_mask].values\n","label_array_test_last = label_array_test_last.reshape(label_array_test_last.shape[0],1).astype(np.float32)\n","label_array_test_last.shape"],"metadata":{"id":"GJCP6XMXCAda"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(seq_array_test_last.shape)\n","print(label_array_test_last.shape)"],"metadata":{"id":"LBiWRPiuCCcR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# test metrics\n","scores_test = model.evaluate(seq_array_test_last, label_array_test_last, verbose=2)\n","print('Accurracy: {}'.format(scores_test[1]))"],"metadata":{"id":"uwNksDFTCEns"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# make predictions and compute confusion matrix\n","y_pred_test = (model.predict(seq_array_test_last) > 0.5).astype(\"int32\")\n","y_true_test = label_array_test_last\n","lstm_rmse = np.sqrt(mean_squared_error(y_pred_test, y_true_test))\n","print(\"The RMSE of LSTM model on test dataset FD001 is \",100*lstm_rmse)\n","#print('Confusion matrix\\n- x-axis is true labels.\\n- y-axis is predicted labels')\n","#cm = confusion_matrix(y_true_test, y_pred_test)\n","#cm"],"metadata":{"id":"F4qUUgK1CGux"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**LSTM** on test data set with **sliding window** has an RMSE of **17.96**"],"metadata":{"id":"CwXwPhUR91Jx"}},{"cell_type":"code","source":["# compute precision and recall\n","precision_test = precision_score(y_true_test, y_pred_test)\n","recall_test = recall_score(y_true_test, y_pred_test)\n","f1_test = 2 * (precision_test * recall_test) / (precision_test + recall_test)\n","print( 'Precision: ', precision_test, '\\n', 'Recall: ', recall_test,'\\n', 'F1-score:', f1_test )"],"metadata":{"id":"oRzbn6Mg-xqT"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"TPU","colab":{"collapsed_sections":["wb00bcxu9tih"],"name":"Aufgabe 8 C_MAPSS_RUL_Estimation .ipynb","private_outputs":true,"provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.7"}},"nbformat":4,"nbformat_minor":0}